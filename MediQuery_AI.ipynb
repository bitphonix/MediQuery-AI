{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "pRyFGJXRNvm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu"
      ],
      "metadata": {
        "id": "6_NNLUUINyfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "Y9wwoT3OOAHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQrPVvGpNCFc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "import gc\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms, models\n",
        "from datasets import load_dataset, Dataset\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel, T5ForConditionalGeneration, T5Tokenizer, TrainingArguments, Trainer\n",
        "import multiprocessing as mp\n",
        "import datetime\n",
        "\n",
        "# === Configuration for Flexibility ===\n",
        "class Config:\n",
        "    IMAGE_MODEL = \"chexnet\"  # Options: \"chexnet\", \"densenet\"\n",
        "    TEXT_MODEL = \"biobert\"   # Options: \"biobert\", \"clinicalbert\"\n",
        "    GEN_MODEL = \"google/flan-t5-base\"  # Options: \"flan-t5-base\", \"flan-t5-large\"\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "# === Data Preprocessing ===\n",
        "class MIMICPreprocessor:\n",
        "    def __init__(self, save_dir=\"<your_local_or_drive_path>\", batch_size=Config.BATCH_SIZE):\n",
        "        self.save_dir = save_dir\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = 224\n",
        "        self.mount_drive()\n",
        "        self.setup_logging()\n",
        "        self.setup_transforms()\n",
        "\n",
        "    def mount_drive(self):\n",
        "        drive.mount('/content/drive')\n",
        "        os.makedirs(self.save_dir, exist_ok=True)\n",
        "        logging.info(f\"Mounted Google Drive and created directory: {self.save_dir}\")\n",
        "\n",
        "    def setup_logging(self):\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[logging.FileHandler(f\"{self.save_dir}/preprocessing.log\"), logging.StreamHandler()]\n",
        "        )\n",
        "\n",
        "    def setup_transforms(self):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((self.image_size, self.image_size)),\n",
        "            transforms.Grayscale(num_output_channels=1),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def process_text(self, findings, impression):\n",
        "        findings = findings or \"No findings available.\"\n",
        "        impression = impression or \"No impression available.\"\n",
        "        combined_text = f\"FINDINGS: {findings}\\nIMPRESSION: {impression}\"\n",
        "        return \" \".join(combined_text.split())\n",
        "\n",
        "    def process_image(self, image):\n",
        "        try:\n",
        "            if not isinstance(image, Image.Image):\n",
        "                raise ValueError(\"Input must be a PIL Image\")\n",
        "            if image.mode != 'L':\n",
        "                image = image.convert('L')\n",
        "            return self.transform(image)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Failed to process image: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_batch(self, batch, batch_idx):\n",
        "        processed_data = {'image_tensors': [], 'combined_text': [], 'valid_indices': []}\n",
        "        for idx in range(len(batch['image'])):\n",
        "            try:\n",
        "                combined_text = self.process_text(batch['findings'][idx], batch['impression'][idx])\n",
        "                img_tensor = self.process_image(batch['image'][idx])\n",
        "                if img_tensor is not None:\n",
        "                    processed_data['image_tensors'].append(img_tensor)\n",
        "                    processed_data['combined_text'].append(combined_text)\n",
        "                    processed_data['valid_indices'].append(idx)\n",
        "            except Exception as e:\n",
        "                logging.warning(f\"Failed to process item {idx} in batch {batch_idx}: {str(e)}\")\n",
        "            if idx % 50 == 0:\n",
        "                gc.collect()\n",
        "                torch.cuda.empty_cache()\n",
        "        return processed_data\n",
        "\n",
        "    def save_preprocessed_data(self, processed_data, batch_idx):\n",
        "        try:\n",
        "            torch.save(torch.stack(processed_data['image_tensors']), f\"{self.save_dir}/images_batch_{batch_idx}.pt\")\n",
        "            pd.DataFrame({\n",
        "                'combined_text': processed_data['combined_text'],\n",
        "                'valid_indices': processed_data['valid_indices']\n",
        "            }).to_csv(f\"{self.save_dir}/text_batch_{batch_idx}.csv\", index=False)\n",
        "            logging.info(f\"Saved batch {batch_idx}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save batch {batch_idx}: {str(e)}\")\n",
        "\n",
        "    def preprocess(self):\n",
        "        print(\"\\n=== Starting MIMIC-CXR Dataset Preprocessing ===\\n\")\n",
        "        dataset = load_dataset(\"itsanmolgupta/mimic-cxr-dataset\")\n",
        "        total_samples = len(dataset['train'])\n",
        "        print(f\"âœ“ Loaded dataset with {total_samples} samples\\n\")\n",
        "        processed_images, failed_images = 0, 0\n",
        "        for batch_idx in tqdm(range(0, total_samples, self.batch_size), desc=\"Processing Batches\"):\n",
        "            batch = dataset['train'][batch_idx:batch_idx + self.batch_size]\n",
        "            processed_data = self.preprocess_batch(batch, batch_idx)\n",
        "            if processed_data['valid_indices']:\n",
        "                self.save_preprocessed_data(processed_data, batch_idx)\n",
        "                processed_images += len(processed_data['valid_indices'])\n",
        "            failed_images += self.batch_size - len(processed_data['valid_indices'])\n",
        "            if batch_idx % (self.batch_size * 10) == 0 and batch_idx > 0:\n",
        "                print(f\"\\nProgress: {processed_images} processed, {failed_images} failed\\n\")\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        print(f\"\\n=== Preprocessing Complete ===\\nTotal: {processed_images + failed_images}, Success: {processed_images}, Failed: {failed_images}\")\n",
        "\n",
        "# === Knowledge Base Creation with Distributed Processing ===\n",
        "class EmbeddingGenerator:\n",
        "    def __init__(self, preprocessed_dir=\"<your_path>\",\n",
        "                 output_dir=\"<your_path>\"):\n",
        "        self.preprocessed_dir = preprocessed_dir\n",
        "        self.output_dir = output_dir\n",
        "        self.device = torch.device(\"cpu\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        self.setup_logging()\n",
        "        self.setup_models()\n",
        "\n",
        "    def setup_logging(self):\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "            handlers=[logging.FileHandler(f\"{self.output_dir}/embedding_generation.log\"), logging.StreamHandler()]\n",
        "        )\n",
        "\n",
        "    def setup_models(self):\n",
        "        logging.info(\"Loading models on CPU...\")\n",
        "        if Config.IMAGE_MODEL == \"chexnet\":\n",
        "            self.image_model = models.densenet121(pretrained=False)\n",
        "            chexnet_url = \"https://github.com/arnoweng/CheXNet/raw/master/model.pth.tar\"\n",
        "            checkpoint = torch.hub.load_state_dict_from_url(chexnet_url, progress=True, map_location=self.device)\n",
        "            state_dict = {k.replace('densenet121.', '').replace('.norm.', '.norm').replace('.conv.', '.conv')\n",
        "                         .replace('.1', '1').replace('.2', '2').replace('classifier.0', 'classifier'): v\n",
        "                         for k, v in checkpoint['state_dict'].items()}\n",
        "            self.image_model.load_state_dict(state_dict, strict=False)\n",
        "        else:\n",
        "            self.image_model = models.densenet121(pretrained=True)\n",
        "        self.image_model = nn.Sequential(*list(self.image_model.children())[:-1]).to(self.device).eval()\n",
        "\n",
        "        text_model_name = \"dmis-lab/biobert-v1.1\" if Config.TEXT_MODEL == \"biobert\" else \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "        try:\n",
        "            self.text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
        "            self.text_model = AutoModel.from_pretrained(text_model_name).to(self.device).eval()\n",
        "            logging.info(f\"{text_model_name} loaded successfully\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to load text model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_text_embedding(self, text):\n",
        "        try:\n",
        "            if len(self.text_tokenizer.encode(text, add_special_tokens=False)) > 512:\n",
        "                logging.warning(f\"Text truncated: {text[:50]}...\")\n",
        "            inputs = self.text_tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                outputs = self.text_model(**inputs)\n",
        "            return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Failed to generate text embedding: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def generate_image_embedding(self, image_tensor):\n",
        "        try:\n",
        "            if image_tensor.dim() != 3 or image_tensor.shape[1:] != (224, 224):\n",
        "                raise ValueError(\"Image tensor must be [C, 224, 224]\")\n",
        "            if image_tensor.shape[0] == 1:\n",
        "                image_tensor = image_tensor.repeat(3, 1, 1)\n",
        "            image_tensor = image_tensor.unsqueeze(0).to(self.device)\n",
        "            with torch.no_grad():\n",
        "                embedding = self.image_model(image_tensor)\n",
        "                embedding = nn.functional.avg_pool2d(embedding, kernel_size=(7, 7)).squeeze().flatten().cpu().numpy()\n",
        "            return embedding.reshape(1, -1)\n",
        "        except Exception as e:\n",
        "            logging.warning(f\"Failed to generate image embedding: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def process_batch(self, args):\n",
        "        image_file, text_file = args\n",
        "        try:\n",
        "            image_path = os.path.join(self.preprocessed_dir, image_file)\n",
        "            text_path = os.path.join(self.preprocessed_dir, text_file)\n",
        "            if not os.path.exists(image_path) or not os.path.exists(text_path):\n",
        "                logging.warning(f\"Missing file: {image_file} or {text_file}\")\n",
        "                return None\n",
        "            image_tensors = torch.load(image_path, map_location=self.device, weights_only=True)\n",
        "            text_data = pd.read_csv(text_path)\n",
        "            image_embeddings, text_embeddings, valid_indices, valid_texts = [], [], [], []\n",
        "            for i in range(len(image_tensors)):\n",
        "                img_emb = self.generate_image_embedding(image_tensors[i])\n",
        "                txt_emb = self.generate_text_embedding(text_data['combined_text'].iloc[i])\n",
        "                if img_emb is not None and txt_emb is not None:\n",
        "                    image_embeddings.append(img_emb[0])\n",
        "                    text_embeddings.append(txt_emb[0])\n",
        "                    valid_indices.append(text_data['valid_indices'].iloc[i])\n",
        "                    valid_texts.append(text_data['combined_text'].iloc[i])\n",
        "            return {\n",
        "                'image_embeddings': np.array(image_embeddings) if image_embeddings else None,\n",
        "                'text_embeddings': np.array(text_embeddings) if text_embeddings else None,\n",
        "                'valid_indices': valid_indices,\n",
        "                'valid_texts': valid_texts\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to process batch {image_file}: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def create_faiss_index(self, embeddings):\n",
        "        try:\n",
        "            dimension = embeddings.shape[1]\n",
        "            index = faiss.IndexFlatL2(dimension)\n",
        "            index.add(embeddings.astype('float32'))\n",
        "            return index\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to create FAISS index: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def generate_embeddings(self, chunk_size=5):\n",
        "        print(\"\\n=== Starting Knowledge Base Creation with Distributed Processing ===\\n\")\n",
        "        image_files = sorted([f for f in os.listdir(self.preprocessed_dir) if f.startswith('images_batch_')])\n",
        "        text_files = sorted([f for f in os.listdir(self.preprocessed_dir) if f.startswith('text_batch_')])\n",
        "        matched_files = list(zip(image_files, text_files))\n",
        "        print(f\"Found {len(matched_files)} batches to process\")\n",
        "\n",
        "        with mp.Pool(processes=mp.cpu_count()) as pool:\n",
        "            results = list(tqdm(pool.imap(self.process_batch, matched_files), total=len(matched_files), desc=\"Processing Batches\"))\n",
        "\n",
        "        all_image_embeddings, all_text_embeddings, all_valid_indices, all_valid_texts = [], [], [], []\n",
        "        for chunk_start in range(0, len(matched_files), chunk_size):\n",
        "            chunk_end = min(chunk_start + chunk_size, len(matched_files))\n",
        "            for result in results[chunk_start:chunk_end]:\n",
        "                if result and result['image_embeddings'] is not None:\n",
        "                    all_image_embeddings.append(result['image_embeddings'])\n",
        "                    all_text_embeddings.append(result['text_embeddings'])\n",
        "                    all_valid_indices.extend(result['valid_indices'])\n",
        "                    all_valid_texts.extend(result['valid_texts'])\n",
        "            if all_image_embeddings:\n",
        "                self.save_intermediate_results(all_image_embeddings, all_text_embeddings, all_valid_indices, all_valid_texts, chunk_start)\n",
        "                all_image_embeddings, all_text_embeddings, all_valid_indices, all_valid_texts = [], [], [], []\n",
        "            gc.collect()\n",
        "        self.merge_and_save_results()\n",
        "\n",
        "    def save_intermediate_results(self, image_embs, text_embs, indices, texts, chunk_id):\n",
        "        try:\n",
        "            intermediate_dir = os.path.join(self.output_dir, \"intermediate\")\n",
        "            os.makedirs(intermediate_dir, exist_ok=True)\n",
        "            np.save(f\"{intermediate_dir}/img_emb_chunk_{chunk_id}.npy\", np.vstack(image_embs))\n",
        "            np.save(f\"{intermediate_dir}/txt_emb_chunk_{chunk_id}.npy\", np.vstack(text_embs))\n",
        "            pd.DataFrame({'valid_index': indices, 'combined_text': texts}).to_csv(\n",
        "                f\"{intermediate_dir}/text_data_chunk_{chunk_id}.csv\", index=False\n",
        "            )\n",
        "            logging.info(f\"Saved chunk {chunk_id}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to save chunk {chunk_id}: {str(e)}\")\n",
        "\n",
        "    def merge_and_save_results(self):\n",
        "        intermediate_dir = os.path.join(self.output_dir, \"intermediate\")\n",
        "        img_emb_files = sorted([f for f in os.listdir(intermediate_dir) if f.startswith('img_emb_chunk_')])\n",
        "        txt_emb_files = sorted([f for f in os.listdir(intermediate_dir) if f.startswith('txt_emb_chunk_')])\n",
        "        text_files = sorted([f for f in os.listdir(intermediate_dir) if f.startswith('text_data_chunk_')])\n",
        "        total_samples = sum(np.load(os.path.join(intermediate_dir, f)).shape[0] for f in img_emb_files)\n",
        "        final_image_embs = np.memmap(f\"{self.output_dir}/image_embeddings_temp.dat\", dtype='float32', mode='w+', shape=(total_samples, 1024))\n",
        "        final_text_embs = np.memmap(f\"{self.output_dir}/text_embeddings_temp.dat\", dtype='float32', mode='w+', shape=(total_samples, 768))\n",
        "        offset = 0\n",
        "        for img_file, txt_file in zip(img_emb_files, txt_emb_files):\n",
        "            img_data = np.load(os.path.join(intermediate_dir, img_file))\n",
        "            txt_data = np.load(os.path.join(intermediate_dir, txt_file))\n",
        "            chunk_size = img_data.shape[0]\n",
        "            final_image_embs[offset:offset + chunk_size] = img_data\n",
        "            final_text_embs[offset:offset + chunk_size] = txt_data\n",
        "            offset += chunk_size\n",
        "            logging.info(f\"Merged chunk: {img_file}\")\n",
        "        np.save(f\"{self.output_dir}/image_embeddings.npy\", final_image_embs)\n",
        "        np.save(f\"{self.output_dir}/text_embeddings.npy\", final_text_embs)\n",
        "        image_index = self.create_faiss_index(final_image_embs)\n",
        "        text_index = self.create_faiss_index(final_text_embs)\n",
        "        final_text_data = pd.concat([pd.read_csv(os.path.join(intermediate_dir, f)) for f in text_files])\n",
        "        faiss.write_index(image_index, f\"{self.output_dir}/image_index.faiss\")\n",
        "        faiss.write_index(text_index, f\"{self.output_dir}/text_index.faiss\")\n",
        "        final_text_data.to_csv(f\"{self.output_dir}/text_data.csv\", index=False)\n",
        "        os.remove(f\"{self.output_dir}/image_embeddings_temp.dat\")\n",
        "        os.remove(f\"{self.output_dir}/text_embeddings_temp.dat\")\n",
        "        print(f\"\\n=== Knowledge Base Creation Complete ===\\nSaved to: {self.output_dir}\")\n",
        "\n",
        "# === Fine-Tuning Data Preparation ===\n",
        "def create_pairs(row):\n",
        "    combined_text = row['combined_text']\n",
        "    findings = combined_text.split('FINDINGS:')[1].strip() if 'FINDINGS:' in combined_text else combined_text\n",
        "    impression = combined_text.split('IMPRESSION:')[1].strip() if 'IMPRESSION:' in combined_text else \"\"\n",
        "    conditions = []\n",
        "    if any(k in combined_text.lower() for k in {\"pneumonia\", \"consolidation\", \"opacity\", \"infiltrate\"}):\n",
        "        conditions.append(\"pneumonia\")\n",
        "    if any(k in combined_text.lower() for k in {\"pulmonary edema\", \"interstitial\", \"effusion\", \"haze\", \"cardiomegaly\"}):\n",
        "        conditions.append(\"pulmonary edema\")\n",
        "    if any(k in combined_text.lower() for k in {\"clear\", \"normal\", \"unremarkable\", \"no consolidation\", \"no effusion\"}):\n",
        "        conditions.append(\"normal\")\n",
        "    if not conditions:\n",
        "        conditions.append(\"unknown\")\n",
        "    condition_str = \", \".join(conditions)\n",
        "    if \"normal\" in conditions:\n",
        "        query = \"What are normal chest X-ray findings?\"\n",
        "        response = f\"\"\"This chest X-ray demonstrates normal findings:\\n1. Radiographic Findings: {findings}\\n2. Key Characteristics: Clear lung fields, normal cardiac silhouette.\\n3. Clinical Significance: No acute abnormalities.\"\"\"\n",
        "    elif \"unknown\" in conditions:\n",
        "        query = f\"Describe the findings in this chest X-ray with {condition_str}.\"\n",
        "        response = f\"\"\"Chest X-ray Analysis:\\n1. Radiographic Findings: {findings}\\n2. Key Observations: {impression if impression else 'No impression provided'}.\\n3. Clinical Significance: Correlate with clinical presentation.\"\"\"\n",
        "    else:\n",
        "        query = f\"What does a chest X-ray with {condition_str} look like?\"\n",
        "        response = f\"\"\"Analysis for {condition_str}:\\n1. Radiographic Findings: {findings}\\n2. Key Characteristics: {condition_str}-specific features.\\n3. Clinical Significance: Suggests {condition_str}, correlate clinically.\"\"\"\n",
        "    return {\"input\": f\"Query: {query}\\nContext: {combined_text}\\nAnswer:\", \"output\": response, \"condition\": condition_str}\n",
        "\n",
        "def validate_responses(dataset, n=5):\n",
        "    for i in range(min(n, len(dataset))):\n",
        "        logging.info(f\"Sample {i}: Query: {dataset['input'][i]}\\nResponse: {dataset['output'][i]}\")\n",
        "\n",
        "def prepare_finetuning_data():\n",
        "    drive.mount('/content/drive')\n",
        "    df = pd.read_csv('<your_path>')\n",
        "    df_sample = df.sample(2000, random_state=42)\n",
        "    data = [create_pairs(row) for _, row in df_sample.iterrows()]\n",
        "    dataset = pd.DataFrame(data)\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    train_data = dataset[:train_size]\n",
        "    eval_data = dataset[train_size:]\n",
        "    validate_responses(train_data)\n",
        "    train_data.to_csv('<your_path>', index=False)\n",
        "    eval_data.to_csv('<your_path>', index=False)\n",
        "    print(f\"Prepared {len(train_data)} training samples and {len(eval_data)} evaluation samples\")\n",
        "\n",
        "# === Fine-Tuning ===\n",
        "def fine_tune_model():\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    device = 'cpu'\n",
        "    train_df = pd.read_csv('<your_path>')\n",
        "    eval_df = pd.read_csv('<your_path>')\n",
        "    train_dataset = Dataset.from_pandas(train_df)\n",
        "    eval_dataset = Dataset.from_pandas(eval_df)\n",
        "    model_name = Config.GEN_MODEL\n",
        "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "    model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        inputs = tokenizer(examples[\"input\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "        outputs = tokenizer(examples[\"output\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "        labels = outputs[\"input_ids\"].copy()\n",
        "        for i in range(len(labels)):\n",
        "            labels[i] = [label if label != tokenizer.pad_token_id else -100 for label in labels[i]]\n",
        "        return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"], \"labels\": labels}\n",
        "\n",
        "    max_train_samples, max_eval_samples = 1600, 400\n",
        "    if len(train_dataset) > max_train_samples:\n",
        "        train_dataset = train_dataset.select(range(max_train_samples))\n",
        "    if len(eval_dataset) > max_eval_samples:\n",
        "        eval_dataset = eval_dataset.select(range(max_eval_samples))\n",
        "    tokenized_train = train_dataset.map(tokenize_function, batched=True, batch_size=200, remove_columns=[\"input\", \"output\", \"condition\"])\n",
        "    tokenized_eval = eval_dataset.map(tokenize_function, batched=True, batch_size=200, remove_columns=[\"input\", \"output\", \"condition\"])\n",
        "    gc.collect()\n",
        "\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_dir = f\"./flan-t5-finetuned-mediquery_cpu_{timestamp}\"\n",
        "    model_save_path = f\"<your_path>/flan-t5-finetuned-mediquery_cpu_{timestamp}\"\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=5,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy=\"steps\",\n",
        "        eval_steps=100,\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=100,\n",
        "        load_best_model_at_end=True,\n",
        "        fp16=False,\n",
        "        gradient_accumulation_steps=8,\n",
        "        dataloader_num_workers=0,\n",
        "        seed=42,\n",
        "        learning_rate=2e-5,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "        remove_unused_columns=False,\n",
        "        label_names=[\"labels\"],\n",
        "        optim=\"adamw_torch\"\n",
        "    )\n",
        "    trainer = Trainer(model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_eval)\n",
        "    trainer.train()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    os.makedirs(os.path.dirname(model_save_path), exist_ok=True)\n",
        "    model.save_pretrained(model_save_path)\n",
        "    tokenizer.save_pretrained(model_save_path)\n",
        "    print(f\"Fine-tuning completed and model saved to {model_save_path}\")\n",
        "\n",
        "# === Execution ===\n",
        "if __name__ == \"__main__\":\n",
        "    logging.getLogger().handlers.clear()\n",
        "    preprocessor = MIMICPreprocessor()\n",
        "    preprocessor.preprocess()\n",
        "    embedding_generator = EmbeddingGenerator()\n",
        "    embedding_generator.generate_embeddings()\n",
        "    prepare_finetuning_data()\n",
        "    fine_tune_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import faiss\n",
        "from transformers import AutoTokenizer, AutoModel, T5ForConditionalGeneration, T5Tokenizer\n",
        "import gradio as gr\n",
        "import cv2\n",
        "import traceback\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "import re\n",
        "import random\n",
        "import functools\n",
        "import gc\n",
        "from collections import OrderedDict\n",
        "from google.colab import drive\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Suppress unnecessary warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "\n",
        "# === Configuration ===\n",
        "class Config:\n",
        "    \"\"\"Configuration for MediQuery system\"\"\"\n",
        "    # Model configuration\n",
        "    IMAGE_MODEL = \"chexnet\"  # Options: \"chexnet\", \"densenet\"\n",
        "    TEXT_MODEL = \"biobert\"   # Options: \"biobert\", \"clinicalbert\"\n",
        "    GEN_MODEL = \"flan-t5-base-finetuned\"  # Base generation model\n",
        "\n",
        "    # Resource management\n",
        "    CACHE_SIZE = 200\n",
        "    CACHE_EXPIRY_TIME = 3600  # Cache expiry time in seconds (1 hour)\n",
        "    LAZY_LOADING = True       # Enable lazy loading of models\n",
        "    USE_HALF_PRECISION = True # Use half precision for models if available\n",
        "\n",
        "    # Feature flags\n",
        "    DEBUG = True              # Enable detailed debugging\n",
        "    PHI_DETECTION_ENABLED = True  # Enable PHI detection\n",
        "    ANATOMY_MAPPING_ENABLED = True  # Enable anatomical mapping\n",
        "\n",
        "    # Thresholds and parameters\n",
        "    CONFIDENCE_THRESHOLD = 0.4  # Threshold for flagging low confidence\n",
        "    TOP_K_RETRIEVAL = 30        # Number of items to retrieve from knowledge base\n",
        "    MAX_CONTEXT_DOCS = 5        # Maximum documents to include in context\n",
        "\n",
        "    # Advanced retrieval settings\n",
        "    DYNAMIC_RERANKING = True    # Dynamically adjust reranking weights\n",
        "    DIVERSITY_PENALTY = 0.1     # Penalty for duplicate content\n",
        "\n",
        "    # Performance optimization\n",
        "    BATCH_SIZE = 4              # Batch size for processing\n",
        "    OPTIMIZE_MEMORY = True      # Optimize memory usage\n",
        "    USE_CACHING = True          # Use caching for embeddings and queries\n",
        "\n",
        "    # UI Settings\n",
        "    IMAGE_HEIGHT = 400  # Increased from 300\n",
        "    IMAGE_WIDTH = 400   # Increased from 300\n",
        "    EXAMPLES_TO_SHOW = 8  # Increased from 6\n",
        "    THEME = \"default\"  # Options: \"default\", \"dark\", \"light\"\n",
        "\n",
        "    # Path settings\n",
        "    DEFAULT_KNOWLEDGE_BASE_DIR = \"<your_path>\"\n",
        "    DEFAULT_MODEL_PATH = \"<your_path>\"\n",
        "    LOG_DIR = \"./logs\"\n",
        "\n",
        "    # Advanced settings\n",
        "    EMBEDDING_AGGREGATION = \"weighted_avg\"  # Options: \"avg\", \"weighted_avg\", \"cls\", \"pooled\"\n",
        "    EMBEDDING_NORMALIZE = True  # Normalize embeddings to unit length\n",
        "\n",
        "    # Error recovery settings\n",
        "    MAX_RETRIES = 3  # Maximum retry attempts for model operations\n",
        "    RECOVERY_WAIT_TIME = 1  # Seconds to wait between retries\n",
        "\n",
        "# Set up logging with improved formatting\n",
        "os.makedirs(Config.LOG_DIR, exist_ok=True)\n",
        "logging.basicConfig(\n",
        "    level=logging.DEBUG if Config.DEBUG else logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(os.path.join(Config.LOG_DIR, f\"mediquery_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(\"MediQuery\")\n",
        "\n",
        "def debug_print(msg):\n",
        "    \"\"\"Print and log debug messages\"\"\"\n",
        "    if Config.DEBUG:\n",
        "        logger.debug(msg)\n",
        "        print(f\"DEBUG: {msg}\")\n",
        "\n",
        "# === Helper Functions for Conditions ===\n",
        "def get_mimic_cxr_conditions():\n",
        "    \"\"\"Return the comprehensive list of conditions in MIMIC-CXR dataset\"\"\"\n",
        "    return [\n",
        "        \"atelectasis\",\n",
        "        \"cardiomegaly\",\n",
        "        \"consolidation\",\n",
        "        \"edema\",\n",
        "        \"enlarged cardiomediastinum\",\n",
        "        \"fracture\",\n",
        "        \"lung lesion\",\n",
        "        \"lung opacity\",\n",
        "        \"no finding\",\n",
        "        \"pleural effusion\",\n",
        "        \"pleural other\",\n",
        "        \"pneumonia\",\n",
        "        \"pneumothorax\",\n",
        "        \"support devices\"\n",
        "    ]\n",
        "\n",
        "def get_condition_synonyms():\n",
        "    \"\"\"Return synonyms for conditions to improve matching\"\"\"\n",
        "    return {\n",
        "        \"atelectasis\": [\"atelectatic change\", \"collapsed lung\", \"lung collapse\"],\n",
        "        \"cardiomegaly\": [\"enlarged heart\", \"cardiac enlargement\", \"heart enlargement\"],\n",
        "        \"consolidation\": [\"airspace opacity\", \"air-space opacity\", \"alveolar opacity\"],\n",
        "        \"edema\": [\"pulmonary edema\", \"fluid overload\", \"vascular congestion\"],\n",
        "        \"fracture\": [\"broken bone\", \"bone fracture\", \"rib fracture\"],\n",
        "        \"lung opacity\": [\"pulmonary opacity\", \"opacification\", \"lung opacification\"],\n",
        "        \"pleural effusion\": [\"pleural fluid\", \"fluid in pleural space\", \"effusion\"],\n",
        "        \"pneumonia\": [\"pulmonary infection\", \"lung infection\", \"bronchopneumonia\"],\n",
        "        \"pneumothorax\": [\"air in pleural space\", \"collapsed lung\", \"ptx\"],\n",
        "        \"support devices\": [\"tube\", \"line\", \"catheter\", \"pacemaker\", \"device\"]\n",
        "    }\n",
        "\n",
        "def get_anatomical_regions():\n",
        "    \"\"\"Return mapping of anatomical regions with descriptions and conditions\"\"\"\n",
        "    return {\n",
        "        \"upper_right_lung\": {\n",
        "            \"description\": \"Upper right lung field\",\n",
        "            \"conditions\": [\"pneumonia\", \"lung lesion\", \"pneumothorax\", \"atelectasis\"]\n",
        "        },\n",
        "        \"upper_left_lung\": {\n",
        "            \"description\": \"Upper left lung field\",\n",
        "            \"conditions\": [\"pneumonia\", \"lung lesion\", \"pneumothorax\", \"atelectasis\"]\n",
        "        },\n",
        "        \"middle_right_lung\": {\n",
        "            \"description\": \"Middle right lung field\",\n",
        "            \"conditions\": [\"pneumonia\", \"lung opacity\", \"atelectasis\"]\n",
        "        },\n",
        "        \"lower_right_lung\": {\n",
        "            \"description\": \"Lower right lung field\",\n",
        "            \"conditions\": [\"pneumonia\", \"pleural effusion\", \"atelectasis\"]\n",
        "        },\n",
        "        \"lower_left_lung\": {\n",
        "            \"description\": \"Lower left lung field\",\n",
        "            \"conditions\": [\"pneumonia\", \"pleural effusion\", \"atelectasis\"]\n",
        "        },\n",
        "        \"heart\": {\n",
        "            \"description\": \"Cardiac silhouette\",\n",
        "            \"conditions\": [\"cardiomegaly\", \"enlarged cardiomediastinum\"]\n",
        "        },\n",
        "        \"hilar\": {\n",
        "            \"description\": \"Hilar regions\",\n",
        "            \"conditions\": [\"enlarged cardiomediastinum\", \"adenopathy\"]\n",
        "        },\n",
        "        \"costophrenic_angles\": {\n",
        "            \"description\": \"Costophrenic angles\",\n",
        "            \"conditions\": [\"pleural effusion\", \"pneumothorax\"]\n",
        "        },\n",
        "        \"spine\": {\n",
        "            \"description\": \"Spine\",\n",
        "            \"conditions\": [\"fracture\", \"degenerative changes\"]\n",
        "        },\n",
        "        \"diaphragm\": {\n",
        "            \"description\": \"Diaphragm\",\n",
        "            \"conditions\": [\"elevated diaphragm\", \"flattened diaphragm\"]\n",
        "        }\n",
        "    }\n",
        "\n",
        "# === PHI Detection and Anonymization ===\n",
        "def detect_phi(text):\n",
        "    \"\"\"Detect potential PHI (Protected Health Information) in text\"\"\"\n",
        "    # Patterns for PHI detection\n",
        "    patterns = {\n",
        "        'name': r'\\b[A-Z][a-z]+ [A-Z][a-z]+\\b',\n",
        "        'mrn': r'\\b[A-Z]{0,3}[0-9]{4,10}\\b',\n",
        "        'ssn': r'\\b[0-9]{3}[-]?[0-9]{2}[-]?[0-9]{4}\\b',\n",
        "        'date': r'\\b(0?[1-9]|1[0-2])[\\/\\-](0?[1-9]|[12]\\d|3[01])[\\/\\-](19|20)\\d{2}\\b',\n",
        "        'phone': r'\\b(\\+\\d{1,2}\\s?)?\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}\\b',\n",
        "        'email': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
        "        'address': r'\\b\\d+\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+\\.?\\b'\n",
        "    }\n",
        "\n",
        "    # Check each pattern\n",
        "    phi_detected = {}\n",
        "    for phi_type, pattern in patterns.items():\n",
        "        matches = re.findall(pattern, text)\n",
        "        if matches:\n",
        "            phi_detected[phi_type] = matches\n",
        "\n",
        "    return phi_detected\n",
        "\n",
        "def anonymize_text(text):\n",
        "    \"\"\"Replace potential PHI with [REDACTED]\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    if not Config.PHI_DETECTION_ENABLED:\n",
        "        return text\n",
        "\n",
        "    try:\n",
        "        # Detect PHI\n",
        "        phi_detected = detect_phi(text)\n",
        "\n",
        "        # Replace PHI with [REDACTED]\n",
        "        anonymized = text\n",
        "        for phi_type, matches in phi_detected.items():\n",
        "            for match in matches:\n",
        "                anonymized = anonymized.replace(match, \"[REDACTED]\")\n",
        "\n",
        "        return anonymized\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in anonymize_text: {str(e)}\")\n",
        "        return text\n",
        "\n",
        "# === LRU Cache Implementation with Enhanced Features ===\n",
        "class LRUCache:\n",
        "    \"\"\"LRU (Least Recently Used) Cache implementation with TTL and size tracking\"\"\"\n",
        "    def __init__(self, capacity=Config.CACHE_SIZE, expiry_time=Config.CACHE_EXPIRY_TIME):\n",
        "        self.cache = OrderedDict()\n",
        "        self.capacity = capacity\n",
        "        self.expiry_time = expiry_time  # in seconds\n",
        "        self.timestamps = {}\n",
        "        self.size_tracking = {\n",
        "            \"current_size_bytes\": 0,\n",
        "            \"max_size_bytes\": 0,\n",
        "            \"items_evicted\": 0,\n",
        "            \"cache_hits\": 0,\n",
        "            \"cache_misses\": 0\n",
        "        }\n",
        "\n",
        "    def get(self, key):\n",
        "        \"\"\"Get item from cache with statistics tracking\"\"\"\n",
        "        if key not in self.cache:\n",
        "            self.size_tracking[\"cache_misses\"] += 1\n",
        "            return None\n",
        "\n",
        "        # Check expiry\n",
        "        if self.is_expired(key):\n",
        "            self._remove_with_tracking(key)\n",
        "            self.size_tracking[\"cache_misses\"] += 1\n",
        "            return None\n",
        "\n",
        "        # Move to end (recently used)\n",
        "        self.size_tracking[\"cache_hits\"] += 1\n",
        "        value = self.cache.pop(key)\n",
        "        self.cache[key] = value\n",
        "        return value\n",
        "\n",
        "    def put(self, key, value):\n",
        "        \"\"\"Add item to cache with size tracking\"\"\"\n",
        "        # Calculate approximate size of the value\n",
        "        value_size = self._estimate_size(value)\n",
        "\n",
        "        if key in self.cache:\n",
        "            old_value = self.cache.pop(key)\n",
        "            old_size = self._estimate_size(old_value)\n",
        "            self.size_tracking[\"current_size_bytes\"] -= old_size\n",
        "\n",
        "        # Make space if needed\n",
        "        while len(self.cache) >= self.capacity or (\n",
        "            Config.OPTIMIZE_MEMORY and\n",
        "            self.size_tracking[\"current_size_bytes\"] + value_size > 1e9  # 1 GB limit\n",
        "        ):\n",
        "            self._evict_least_recently_used()\n",
        "\n",
        "        # Add new item and timestamp\n",
        "        self.cache[key] = value\n",
        "        self.timestamps[key] = datetime.now().timestamp()\n",
        "        self.size_tracking[\"current_size_bytes\"] += value_size\n",
        "\n",
        "        # Update max size\n",
        "        if self.size_tracking[\"current_size_bytes\"] > self.size_tracking[\"max_size_bytes\"]:\n",
        "            self.size_tracking[\"max_size_bytes\"] = self.size_tracking[\"current_size_bytes\"]\n",
        "\n",
        "    def is_expired(self, key):\n",
        "        \"\"\"Check if item has expired\"\"\"\n",
        "        if key not in self.timestamps:\n",
        "            return True\n",
        "\n",
        "        current_time = datetime.now().timestamp()\n",
        "        return (current_time - self.timestamps[key]) > self.expiry_time\n",
        "\n",
        "    def _evict_least_recently_used(self):\n",
        "        \"\"\"Remove least recently used item with tracking\"\"\"\n",
        "        if not self.cache:\n",
        "            return\n",
        "\n",
        "        # Get oldest item\n",
        "        key, value = self.cache.popitem(last=False)\n",
        "        # Remove from timestamps and update tracking\n",
        "        self._remove_with_tracking(key)\n",
        "\n",
        "    def _remove_with_tracking(self, key):\n",
        "        \"\"\"Remove item with size tracking\"\"\"\n",
        "        if key in self.cache:\n",
        "            value = self.cache.pop(key)\n",
        "            value_size = self._estimate_size(value)\n",
        "            self.size_tracking[\"current_size_bytes\"] -= value_size\n",
        "            self.size_tracking[\"items_evicted\"] += 1\n",
        "\n",
        "        if key in self.timestamps:\n",
        "            self.timestamps.pop(key)\n",
        "\n",
        "    def remove(self, key):\n",
        "        \"\"\"Remove item from cache\"\"\"\n",
        "        self._remove_with_tracking(key)\n",
        "\n",
        "    def clear(self):\n",
        "        \"\"\"Clear the cache\"\"\"\n",
        "        self.cache.clear()\n",
        "        self.timestamps.clear()\n",
        "        self.size_tracking[\"current_size_bytes\"] = 0\n",
        "\n",
        "    def get_stats(self):\n",
        "        \"\"\"Get cache statistics\"\"\"\n",
        "        return {\n",
        "            \"size_bytes\": self.size_tracking[\"current_size_bytes\"],\n",
        "            \"max_size_bytes\": self.size_tracking[\"max_size_bytes\"],\n",
        "            \"items\": len(self.cache),\n",
        "            \"capacity\": self.capacity,\n",
        "            \"items_evicted\": self.size_tracking[\"items_evicted\"],\n",
        "            \"hit_rate\": self.size_tracking[\"cache_hits\"] /\n",
        "                        (self.size_tracking[\"cache_hits\"] + self.size_tracking[\"cache_misses\"] + 1e-8)\n",
        "        }\n",
        "\n",
        "    def _estimate_size(self, obj):\n",
        "        \"\"\"Estimate memory size of an object in bytes\"\"\"\n",
        "        if obj is None:\n",
        "            return 0\n",
        "\n",
        "        if isinstance(obj, np.ndarray):\n",
        "            return obj.nbytes\n",
        "        elif isinstance(obj, torch.Tensor):\n",
        "            return obj.element_size() * obj.nelement()\n",
        "        elif isinstance(obj, (str, bytes)):\n",
        "            return len(obj)\n",
        "        elif isinstance(obj, (list, tuple)):\n",
        "            return sum(self._estimate_size(x) for x in obj)\n",
        "        elif isinstance(obj, dict):\n",
        "            return sum(self._estimate_size(k) + self._estimate_size(v) for k, v in obj.items())\n",
        "        else:\n",
        "            # Fallback - rough estimate\n",
        "            return sys.getsizeof(obj)\n",
        "\n",
        "# === Improved Lazy Model Loading ===\n",
        "class LazyModel:\n",
        "    \"\"\"Lazy loading wrapper for models with proper method forwarding and error recovery\"\"\"\n",
        "    def __init__(self, model_name, model_class, device, **kwargs):\n",
        "        self.model_name = model_name\n",
        "        self.model_class = model_class\n",
        "        self.device = device\n",
        "        self.kwargs = kwargs\n",
        "        self._model = None\n",
        "        self.last_error = None\n",
        "        self.last_used = datetime.now()\n",
        "        debug_print(f\"LazyModel initialized for {model_name}\")\n",
        "\n",
        "    def _ensure_loaded(self, retries=Config.MAX_RETRIES):\n",
        "        \"\"\"Ensure model is loaded with retry mechanism\"\"\"\n",
        "        if self._model is None:\n",
        "            debug_print(f\"Lazy loading model: {self.model_name}\")\n",
        "            for attempt in range(retries):\n",
        "                try:\n",
        "                    self._model = self.model_class.from_pretrained(self.model_name, **self.kwargs)\n",
        "\n",
        "                    # Apply memory optimizations\n",
        "                    if Config.OPTIMIZE_MEMORY:\n",
        "                        # Convert to half precision if available and enabled\n",
        "                        if Config.USE_HALF_PRECISION and self.device.type == 'cuda' and hasattr(self._model, 'half'):\n",
        "                            self._model = self._model.half()\n",
        "                            debug_print(f\"Using half precision for {self.model_name}\")\n",
        "\n",
        "                    self._model = self._model.to(self.device)\n",
        "                    self._model.eval()  # Set to evaluation mode\n",
        "                    debug_print(f\"Model {self.model_name} loaded successfully\")\n",
        "                    self.last_error = None\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    self.last_error = str(e)\n",
        "                    debug_print(f\"Error loading model {self.model_name} (attempt {attempt+1}/{retries}): {str(e)}\")\n",
        "                    if attempt < retries - 1:\n",
        "                        # Wait before retrying\n",
        "                        time.sleep(Config.RECOVERY_WAIT_TIME)\n",
        "                    else:\n",
        "                        raise RuntimeError(f\"Failed to load model {self.model_name} after {retries} attempts: {str(e)}\")\n",
        "\n",
        "        # Update last used timestamp\n",
        "        self.last_used = datetime.now()\n",
        "        return self._model\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \"\"\"Call the model\"\"\"\n",
        "        model = self._ensure_loaded()\n",
        "        return model(*args, **kwargs)\n",
        "\n",
        "    # Forward common model methods\n",
        "    def generate(self, *args, **kwargs):\n",
        "        \"\"\"Forward generate method to model with error recovery\"\"\"\n",
        "        model = self._ensure_loaded()\n",
        "        try:\n",
        "            return model.generate(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            # If generation fails, try reloading the model once\n",
        "            debug_print(f\"Generation failed, reloading model: {str(e)}\")\n",
        "            self.unload()\n",
        "            model = self._ensure_loaded()\n",
        "            return model.generate(*args, **kwargs)\n",
        "\n",
        "    def to(self, device):\n",
        "        \"\"\"Move model to specified device\"\"\"\n",
        "        self.device = device\n",
        "        if self._model is not None:\n",
        "            self._model = self._model.to(device)\n",
        "        return self\n",
        "\n",
        "    def eval(self):\n",
        "        \"\"\"Set model to evaluation mode\"\"\"\n",
        "        if self._model is not None:\n",
        "            self._model = self._model.eval()\n",
        "        return self\n",
        "\n",
        "    def unload(self):\n",
        "        \"\"\"Unload model to free memory\"\"\"\n",
        "        if self._model is not None:\n",
        "            debug_print(f\"Unloading model {self.model_name}\")\n",
        "            self._model = None\n",
        "            # Force garbage collection\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    def is_loaded(self):\n",
        "        \"\"\"Check if model is loaded\"\"\"\n",
        "        return self._model is not None\n",
        "\n",
        "# === MIMIC-CXR Index Builder ===\n",
        "def build_mimic_cxr_index(mimic_meta_path=None):\n",
        "    \"\"\"Build an index from MIMIC-CXR metadata or create a synthetic one\"\"\"\n",
        "    debug_print(\"Building MIMIC-CXR index (synthetic version)\")\n",
        "\n",
        "    try:\n",
        "        # Check if text_data.csv exists and use it instead\n",
        "        text_data_path = os.path.join(os.path.dirname(mimic_meta_path), \"text_data.csv\")\n",
        "        if os.path.exists(text_data_path):\n",
        "            debug_print(f\"Using text_data.csv instead of mimic_meta.csv\")\n",
        "            meta_df = pd.read_csv(text_data_path)\n",
        "            debug_print(f\"Loaded text_data with {len(meta_df)} rows\")\n",
        "\n",
        "            # Extract metadata from text_data.csv\n",
        "            # Assuming text_data.csv has a combined_text column\n",
        "            conditions = get_mimic_cxr_conditions()\n",
        "            condition_synonyms = get_condition_synonyms()\n",
        "\n",
        "            # Build index by condition\n",
        "            index_by_condition = {condition: [] for condition in conditions}\n",
        "            index_by_id = {}\n",
        "\n",
        "            # Use row indices as pseudo-image IDs\n",
        "            for idx, row in meta_df.iterrows():\n",
        "                pseudo_id = f\"img_{idx}\"\n",
        "                text = row.get('combined_text', '')\n",
        "\n",
        "                # Store in index_by_id\n",
        "                index_by_id[pseudo_id] = {\n",
        "                    'findings': text,\n",
        "                    'impression': '',  # We don't have separate impression\n",
        "                    'subject_id': pseudo_id,\n",
        "                    'study_id': pseudo_id\n",
        "                }\n",
        "\n",
        "                # Check for conditions in the text\n",
        "                text_lower = text.lower()\n",
        "                for condition in conditions:\n",
        "                    if condition in text_lower:\n",
        "                        index_by_condition[condition].append(pseudo_id)\n",
        "                        continue\n",
        "\n",
        "                    # Check synonyms\n",
        "                    if condition in condition_synonyms:\n",
        "                        for synonym in condition_synonyms[condition]:\n",
        "                            if synonym in text_lower:\n",
        "                                index_by_condition[condition].append(pseudo_id)\n",
        "                                break\n",
        "\n",
        "            # Debug output\n",
        "            for condition in conditions:\n",
        "                count = len(index_by_condition[condition])\n",
        "                debug_print(f\"Found {count} examples for {condition}\")\n",
        "\n",
        "            return {\n",
        "                \"by_image_id\": index_by_id,\n",
        "                \"by_condition\": index_by_condition\n",
        "            }\n",
        "        else:\n",
        "            debug_print(\"Neither mimic_meta.csv nor text_data.csv found, using fully synthetic index\")\n",
        "            return {\"by_image_id\": {}, \"by_condition\": {}}\n",
        "\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error building MIMIC-CXR index: {str(e)}\")\n",
        "        debug_print(traceback.format_exc())\n",
        "        return {\"by_image_id\": {}, \"by_condition\": {}}\n",
        "\n",
        "# === Performance Monitoring Decorator ===\n",
        "def performance_monitor(func):\n",
        "    \"\"\"Decorator to monitor function performance\"\"\"\n",
        "    @functools.wraps(func)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        # Get instance if it's a method\n",
        "        instance = args[0] if args and hasattr(args[0], '__class__') else None\n",
        "\n",
        "        start_time = time.time()\n",
        "        start_memory = 0\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            start_memory = torch.cuda.memory_allocated()\n",
        "\n",
        "        try:\n",
        "            result = func(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "            end_time = time.time()\n",
        "            # Log performance even on failure\n",
        "            func_name = func.__name__\n",
        "            elapsed = end_time - start_time\n",
        "            debug_print(f\"ERROR in {func_name}: {str(e)}, took {elapsed:.2f} seconds\")\n",
        "\n",
        "            # Store metrics if available\n",
        "            if instance and hasattr(instance, 'performance_metrics'):\n",
        "                if func_name not in instance.performance_metrics:\n",
        "                    instance.performance_metrics[func_name] = []\n",
        "                instance.performance_metrics[func_name].append(elapsed)\n",
        "            raise  # Re-raise the exception\n",
        "\n",
        "        end_time = time.time()\n",
        "        end_memory = 0\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "            end_memory = torch.cuda.memory_allocated()\n",
        "            memory_diff = (end_memory - start_memory) / (1024 * 1024)  # Convert to MB\n",
        "        else:\n",
        "            memory_diff = 0\n",
        "\n",
        "        elapsed = end_time - start_time\n",
        "\n",
        "        # Store metrics\n",
        "        if instance and hasattr(instance, 'performance_metrics'):\n",
        "            if func.__name__ not in instance.performance_metrics:\n",
        "                instance.performance_metrics[func.__name__] = []\n",
        "            instance.performance_metrics[func.__name__].append(elapsed)\n",
        "\n",
        "        # Log performance info\n",
        "        if elapsed > 1.0:  # Only log if took more than 1 second\n",
        "            memory_str = f\", memory usage: {memory_diff:.1f} MB\" if torch.cuda.is_available() else \"\"\n",
        "            debug_print(f\"Function {func.__name__} took {elapsed:.2f} seconds{memory_str}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    return wrapper\n",
        "\n",
        "# === Main RAG System Class ===\n",
        "class MediQueryRAG:\n",
        "    def __init__(self, knowledge_base_dir=Config.DEFAULT_KNOWLEDGE_BASE_DIR,\n",
        "                 finetuned_model_path=Config.DEFAULT_MODEL_PATH):\n",
        "        \"\"\"Initialize with enhanced features\"\"\"\n",
        "        self.knowledge_base_dir = knowledge_base_dir\n",
        "        self.finetuned_model_path = finetuned_model_path\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        debug_print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Initialize evaluation metrics\n",
        "        self.eval_metrics = {\n",
        "            \"precision\": [],\n",
        "            \"recall\": [],\n",
        "            \"f1\": []\n",
        "        }\n",
        "\n",
        "        # Initialize performance metrics\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "        # Initialize cache\n",
        "        self.cache = LRUCache(capacity=Config.CACHE_SIZE, expiry_time=Config.CACHE_EXPIRY_TIME)\n",
        "\n",
        "        # Confidence calibration data\n",
        "        self.confidence_calibration = {\n",
        "            0.1: 0.03,\n",
        "            0.2: 0.08,\n",
        "            0.3: 0.15,\n",
        "            0.4: 0.25,\n",
        "            0.5: 0.40,\n",
        "            0.6: 0.58,\n",
        "            0.7: 0.72,\n",
        "            0.8: 0.88,\n",
        "            0.9: 0.95,\n",
        "            1.0: 0.99\n",
        "        }\n",
        "\n",
        "        # Load MIMIC-CXR metadata - Use text_data.csv directly instead of non-existent mimic_meta.csv\n",
        "        self.mimic_meta_path = os.path.join(knowledge_base_dir, \"text_data.csv\")\n",
        "        self.mimic_index = build_mimic_cxr_index(self.mimic_meta_path)\n",
        "\n",
        "        # Track if we have valid MIMIC-CXR data\n",
        "        self.has_mimic_data = \"error\" not in self.mimic_index\n",
        "\n",
        "        if self.has_mimic_data:\n",
        "            print(\"Successfully integrated MIMIC-CXR data\")\n",
        "        else:\n",
        "            print(\"Warning: MIMIC-CXR data unavailable, using synthetic ground truth\")\n",
        "\n",
        "        # Load knowledge base and models\n",
        "        self.load_knowledge_base()\n",
        "        self.load_models()\n",
        "\n",
        "    @performance_monitor\n",
        "    def load_knowledge_base(self):\n",
        "        debug_print(\"Loading knowledge base...\")\n",
        "        try:\n",
        "            self.image_embs = np.load(f\"{self.knowledge_base_dir}/image_embeddings.npy\")\n",
        "            self.text_embs = np.load(f\"{self.knowledge_base_dir}/text_embeddings.npy\")\n",
        "            self.text_data = pd.read_csv(f\"{self.knowledge_base_dir}/text_data.csv\")\n",
        "            self.image_index = faiss.read_index(f\"{self.knowledge_base_dir}/image_index.faiss\")\n",
        "            self.text_index = faiss.read_index(f\"{self.knowledge_base_dir}/text_index.faiss\")\n",
        "            self.image_distances = np.linalg.norm(self.image_embs - self.image_embs.mean(axis=0), axis=1)\n",
        "            debug_print(f\"Knowledge base loaded. Image embeddings shape: {self.image_embs.shape}, Text embeddings shape: {self.text_embs.shape}\")\n",
        "\n",
        "            # Enhanced optimization: Normalize embeddings if enabled\n",
        "            if Config.EMBEDDING_NORMALIZE:\n",
        "                debug_print(\"Normalizing embeddings for better retrieval\")\n",
        "                self.image_embs = self.image_embs / (np.linalg.norm(self.image_embs, axis=1, keepdims=True) + 1e-8)\n",
        "                self.text_embs = self.text_embs / (np.linalg.norm(self.text_embs, axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "            # Load or create anatomical region mapping\n",
        "            if Config.ANATOMY_MAPPING_ENABLED:\n",
        "                self.anatomical_regions = get_anatomical_regions()\n",
        "                debug_print(f\"Loaded {len(self.anatomical_regions)} anatomical regions for mapping\")\n",
        "\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error loading knowledge base: {str(e)}\"\n",
        "            debug_print(error_msg)\n",
        "            raise RuntimeError(error_msg)\n",
        "\n",
        "    @performance_monitor\n",
        "    def load_models(self):\n",
        "        debug_print(\"Loading models...\")\n",
        "        try:\n",
        "            # Load image model (CheXNet or DenseNet)\n",
        "            if Config.LAZY_LOADING:\n",
        "                debug_print(\"Using lazy loading for models\")\n",
        "\n",
        "                # Setup image model\n",
        "                if Config.IMAGE_MODEL == \"chexnet\":\n",
        "                    self.image_model = models.densenet121(weights=None)\n",
        "                    chexnet_url = \"https://github.com/arnoweng/CheXNet/raw/master/model.pth.tar\"\n",
        "                    checkpoint = torch.hub.load_state_dict_from_url(chexnet_url, progress=True, map_location=self.device)\n",
        "                    state_dict = {k.replace('densenet121.', '').replace('.norm.', '.norm').replace('.conv.', '.conv')\n",
        "                                 .replace('.1', '1').replace('.2', '2').replace('classifier.0', 'classifier'): v\n",
        "                                 for k, v in checkpoint['state_dict'].items()}\n",
        "                    self.image_model.load_state_dict(state_dict, strict=False)\n",
        "                else:\n",
        "                    self.image_model = models.densenet121(weights=None)\n",
        "\n",
        "                # Remove classifier to get features\n",
        "                self.image_model = nn.Sequential(*list(self.image_model.children())[:-1]).to(self.device).eval()\n",
        "\n",
        "                # Setup text model with lazy loading\n",
        "                text_model_name = \"dmis-lab/biobert-v1.1\" if Config.TEXT_MODEL == \"biobert\" else \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "                self.text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
        "                self.text_model = LazyModel(text_model_name, AutoModel, self.device)\n",
        "\n",
        "                # Setup generator model with lazy loading\n",
        "                self.gen_tokenizer = T5Tokenizer.from_pretrained(self.finetuned_model_path)\n",
        "                self.gen_model = LazyModel(self.finetuned_model_path, T5ForConditionalGeneration, self.device)\n",
        "            else:\n",
        "                # Load all models eagerly\n",
        "                if Config.IMAGE_MODEL == \"chexnet\":\n",
        "                    self.image_model = models.densenet121(weights=None)\n",
        "                    chexnet_url = \"https://github.com/arnoweng/CheXNet/raw/master/model.pth.tar\"\n",
        "                    checkpoint = torch.hub.load_state_dict_from_url(chexnet_url, progress=True, map_location=self.device)\n",
        "                    state_dict = {k.replace('densenet121.', '').replace('.norm.', '.norm').replace('.conv.', '.conv')\n",
        "                                 .replace('.1', '1').replace('.2', '2').replace('classifier.0', 'classifier'): v\n",
        "                                 for k, v in checkpoint['state_dict'].items()}\n",
        "                    self.image_model.load_state_dict(state_dict, strict=False)\n",
        "                else:\n",
        "                    self.image_model = models.densenet121(weights=None)\n",
        "\n",
        "                # Remove classifier to get features\n",
        "                # Remove classifier to get features\n",
        "                self.image_model = nn.Sequential(*list(self.image_model.children())[:-1]).to(self.device).eval()\n",
        "\n",
        "                # Load text model (BioBERT or ClinicalBERT)\n",
        "                text_model_name = \"dmis-lab/biobert-v1.1\" if Config.TEXT_MODEL == \"biobert\" else \"emilyalsentzer/Bio_ClinicalBERT\"\n",
        "                self.text_tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
        "                self.text_model = AutoModel.from_pretrained(text_model_name).to(self.device).eval()\n",
        "\n",
        "                # Load finetuned generator model\n",
        "                self.gen_tokenizer = T5Tokenizer.from_pretrained(self.finetuned_model_path)\n",
        "                self.gen_model = T5ForConditionalGeneration.from_pretrained(self.finetuned_model_path).to(self.device).eval()\n",
        "\n",
        "                # Apply half precision if enabled\n",
        "                if Config.USE_HALF_PRECISION and self.device.type == 'cuda':\n",
        "                    debug_print(\"Using half precision for models\")\n",
        "                    self.text_model = self.text_model.half()\n",
        "                    self.gen_model = self.gen_model.half()\n",
        "\n",
        "            # Initialize transforms for image preprocessing\n",
        "            self.transform = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "            debug_print(\"Models loaded successfully\")\n",
        "        except Exception as e:\n",
        "            error_msg = f\"Error loading models: {str(e)}\"\n",
        "            debug_print(error_msg)\n",
        "            debug_print(traceback.format_exc())\n",
        "            raise RuntimeError(error_msg)\n",
        "\n",
        "    @performance_monitor\n",
        "    def generate_text_embedding(self, text):\n",
        "        \"\"\"Generate embedding for text query with caching and optimization\"\"\"\n",
        "        debug_print(f\"Generating text embedding for: {text[:50]}...\")\n",
        "\n",
        "        # Check cache first\n",
        "        cache_key = f\"text_emb_{hash(text)}\"\n",
        "        cached_emb = self.cache.get(cache_key)\n",
        "        if cached_emb is not None:\n",
        "            debug_print(\"Using cached text embedding\")\n",
        "            return cached_emb\n",
        "\n",
        "        try:\n",
        "            # Tokenize with retry logic\n",
        "            for attempt in range(Config.MAX_RETRIES):\n",
        "                try:\n",
        "                    inputs = self.text_tokenizer(text, padding=True, truncation=True,\n",
        "                                               return_tensors=\"pt\", max_length=512).to(self.device)\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    if attempt == Config.MAX_RETRIES - 1:\n",
        "                        debug_print(f\"Failed to tokenize text after {Config.MAX_RETRIES} attempts: {str(e)}\")\n",
        "                        raise\n",
        "                    debug_print(f\"Tokenization error (attempt {attempt+1}): {str(e)}\")\n",
        "                    time.sleep(Config.RECOVERY_WAIT_TIME)\n",
        "\n",
        "            # Generate embedding\n",
        "            with torch.no_grad():\n",
        "                # If using LazyModel, the call will trigger lazy loading\n",
        "                if isinstance(self.text_model, LazyModel):\n",
        "                    outputs = self.text_model(**inputs)\n",
        "                else:\n",
        "                    outputs = self.text_model(**inputs)\n",
        "\n",
        "                # Different aggregation strategies\n",
        "                if Config.EMBEDDING_AGGREGATION == \"cls\":\n",
        "                    embedding = outputs.last_hidden_state[:, 0].cpu().numpy()  # Use CLS token\n",
        "                elif Config.EMBEDDING_AGGREGATION == \"pooled\":\n",
        "                    if hasattr(outputs, 'pooler_output'):\n",
        "                        embedding = outputs.pooler_output.cpu().numpy()  # Use pooler output\n",
        "                    else:\n",
        "                        embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "                elif Config.EMBEDDING_AGGREGATION == \"weighted_avg\":\n",
        "                    # Use weighted average of last hidden states (attention-based)\n",
        "                    last_hidden = outputs.last_hidden_state\n",
        "                    weights = torch.softmax(torch.sum(last_hidden * inputs['input_ids'].unsqueeze(-1), dim=-1), dim=1)\n",
        "                    embedding = (last_hidden * weights.unsqueeze(-1)).sum(dim=1).cpu().numpy()\n",
        "                else:  # Default to average pooling\n",
        "                    embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
        "\n",
        "            debug_print(f\"Text embedding shape: {embedding.shape}\")\n",
        "\n",
        "            # Normalize embedding if configured\n",
        "            if Config.EMBEDDING_NORMALIZE:\n",
        "                embedding = embedding / (np.linalg.norm(embedding, axis=1, keepdims=True) + 1e-8)\n",
        "\n",
        "            # Cache the result\n",
        "            self.cache.put(cache_key, embedding)\n",
        "\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in generate_text_embedding: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            raise\n",
        "\n",
        "    @performance_monitor\n",
        "    def generate_image_embedding(self, image_tensor):\n",
        "        \"\"\"Generate embedding for image with enhanced attention weight extraction\"\"\"\n",
        "        debug_print(f\"Generating image embedding. Input type: {type(image_tensor)}\")\n",
        "\n",
        "        # Check cache if we have image hash\n",
        "        if hasattr(image_tensor, 'filename'):\n",
        "            cache_key = f\"img_emb_{hash(image_tensor.filename)}\"\n",
        "            cached_emb = self.cache.get(cache_key)\n",
        "            if cached_emb is not None:\n",
        "                debug_print(\"Using cached image embedding\")\n",
        "                return cached_emb\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.Grayscale(num_output_channels=1),\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "        try:\n",
        "            # Handle different input types\n",
        "            if isinstance(image_tensor, Image.Image):\n",
        "                debug_print(f\"Input is PIL Image with mode: {image_tensor.mode}\")\n",
        "                if image_tensor.mode == 'RGB':\n",
        "                    image_tensor = transform(image_tensor.convert('L'))\n",
        "                else:\n",
        "                    image_tensor = transform(image_tensor)\n",
        "            elif isinstance(image_tensor, torch.Tensor):\n",
        "                debug_print(f\"Input is torch.Tensor with shape: {image_tensor.shape}\")\n",
        "                if image_tensor.dim() == 3:\n",
        "                    if image_tensor.shape[0] == 3:  # RGB tensor in CHW format\n",
        "                        image_tensor = transform(Image.fromarray((image_tensor.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)))\n",
        "                    elif image_tensor.shape[-1] == 3:  # RGB tensor in HWC format\n",
        "                        image_tensor = transform(Image.fromarray((image_tensor.cpu().numpy() * 255).astype(np.uint8)))\n",
        "                    else:  # Single-channel\n",
        "                        image_tensor = transform(Image.fromarray((image_tensor.squeeze().cpu().numpy() * 255).astype(np.uint8)))\n",
        "                elif image_tensor.dim() == 2:  # Grayscale\n",
        "                    image_tensor = transform(Image.fromarray((image_tensor.cpu().numpy() * 255).astype(np.uint8)))\n",
        "            elif isinstance(image_tensor, np.ndarray):\n",
        "                debug_print(f\"Input is numpy.ndarray with shape: {image_tensor.shape}\")\n",
        "                if len(image_tensor.shape) == 3 and image_tensor.shape[-1] == 3:  # RGB array\n",
        "                    image_tensor = transform(Image.fromarray(image_tensor))\n",
        "                else:  # Grayscale or single-channel\n",
        "                    image_tensor = transform(Image.fromarray(image_tensor))\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected image tensor type: {type(image_tensor)}\")\n",
        "\n",
        "            # Ensure 3 channels for model\n",
        "            if image_tensor.shape[0] == 1:\n",
        "                image_tensor = image_tensor.repeat(3, 1, 1)\n",
        "\n",
        "            image_tensor = image_tensor.unsqueeze(0).to(self.device)\n",
        "            debug_print(f\"Processed image tensor shape: {image_tensor.shape}\")\n",
        "\n",
        "            with torch.no_grad():\n",
        "                # Extract features with hooks for attention weights\n",
        "                activation = {}\n",
        "                def get_activation(name):\n",
        "                    def hook(model, input, output):\n",
        "                        activation[name] = output.detach()\n",
        "                    return hook\n",
        "\n",
        "                # Register forward hook on last convolutional layer to extract attention\n",
        "                if not isinstance(self.image_model, LazyModel):\n",
        "                    # Find the last conv layer for hook registration\n",
        "                    for name, layer in self.image_model.named_modules():\n",
        "                        if isinstance(layer, nn.Conv2d):\n",
        "                            last_conv_name = name\n",
        "\n",
        "                    # Get last conv layer for attention extraction\n",
        "                    for name, layer in self.image_model.named_modules():\n",
        "                        if name == last_conv_name:\n",
        "                            layer.register_forward_hook(get_activation('features'))\n",
        "\n",
        "                # Get features\n",
        "                features = self.image_model(image_tensor)\n",
        "\n",
        "                # Enhanced attention weight extraction\n",
        "                if 'features' in activation:\n",
        "                    feature_activations = activation['features'].squeeze()\n",
        "                    # Save raw activations for detailed analysis\n",
        "                    self.raw_activations = feature_activations.cpu().numpy()\n",
        "\n",
        "                    # Calculate attention weights with softmax to enhance key areas\n",
        "                    if feature_activations.dim() > 1:\n",
        "                        attention_weights = nn.functional.softmax(feature_activations.flatten(), dim=0).reshape(feature_activations.shape)\n",
        "                        self.last_attention_weights = attention_weights.cpu().numpy()\n",
        "\n",
        "                        # Extract spatial attention patterns\n",
        "                        h, w = attention_weights.shape[-2:]\n",
        "                        # Calculate attention by regions\n",
        "                        self.region_attention = {\n",
        "                            'upper_left': attention_weights[:, :h//2, :w//2].mean().item(),\n",
        "                            'upper_right': attention_weights[:, :h//2, w//2:].mean().item(),\n",
        "                            'lower_left': attention_weights[:, h//2:, :w//2].mean().item(),\n",
        "                            'lower_right': attention_weights[:, h//2:, w//2:].mean().item(),\n",
        "                            'central': attention_weights[:, h//4:3*h//4, w//4:3*w//4].mean().item()\n",
        "                        }\n",
        "                    else:\n",
        "                        self.last_attention_weights = np.ones((7, 7))\n",
        "                        self.region_attention = {\n",
        "                            'upper_left': 0.2, 'upper_right': 0.2,\n",
        "                            'lower_left': 0.2, 'lower_right': 0.2, 'central': 0.2\n",
        "                        }\n",
        "                else:\n",
        "                    # Fallback for LazyModel or if hook failed\n",
        "                    feature_map_size = int(np.sqrt(features.shape[1]))\n",
        "                    self.last_attention_weights = np.ones((feature_map_size, feature_map_size))\n",
        "                    self.region_attention = {\n",
        "                        'upper_left': 0.2, 'upper_right': 0.2,\n",
        "                        'lower_left': 0.2, 'lower_right': 0.2, 'central': 0.2\n",
        "                    }\n",
        "\n",
        "                # Global pooling for embedding\n",
        "                pooled = nn.functional.adaptive_avg_pool2d(features, (1, 1)).flatten()\n",
        "\n",
        "            # Extract embedded anatomical regions\n",
        "            self.detect_anatomical_regions()\n",
        "\n",
        "            embedding = pooled.cpu().numpy().reshape(1, -1)\n",
        "            debug_print(f\"Image embedding shape: {embedding.shape}\")\n",
        "\n",
        "            # Normalize embedding if configured\n",
        "            if Config.EMBEDDING_NORMALIZE:\n",
        "                embedding = embedding / (np.linalg.norm(embedding) + 1e-8)\n",
        "\n",
        "            # Cache the embedding if we have a filename\n",
        "            if hasattr(image_tensor, 'filename'):\n",
        "                self.cache.put(cache_key, embedding)\n",
        "\n",
        "            return embedding\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in generate_image_embedding: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            raise\n",
        "\n",
        "    def detect_anatomical_regions(self):\n",
        "        \"\"\"Detect anatomical regions based on attention weights\"\"\"\n",
        "        if not hasattr(self, 'last_attention_weights') or not Config.ANATOMY_MAPPING_ENABLED:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            debug_print(\"Detecting anatomical regions from attention weights\")\n",
        "\n",
        "            # Get attention map\n",
        "            attention_map = self.last_attention_weights\n",
        "\n",
        "            # Initialize region detection results\n",
        "            self.detected_regions = {}\n",
        "\n",
        "            # Map regions based on spatial positions\n",
        "            h, w = attention_map.shape[:2] if len(attention_map.shape) > 1 else (7, 7)\n",
        "\n",
        "            # Define region coordinates (normalized to attention map size)\n",
        "            regions = {\n",
        "                \"upper_right_lung\": (slice(0, h//3), slice(2*w//3, w)),\n",
        "                \"upper_left_lung\": (slice(0, h//3), slice(0, w//3)),\n",
        "                \"middle_right_lung\": (slice(h//3, 2*h//3), slice(2*w//3, w)),\n",
        "                \"lower_right_lung\": (slice(2*h//3, h), slice(2*w//3, w)),\n",
        "                \"lower_left_lung\": (slice(2*h//3, h), slice(0, w//3)),\n",
        "                \"heart\": (slice(h//3, 2*h//3), slice(w//3, 2*w//3)),\n",
        "                \"hilar\": (slice(h//4, h//2), slice(w//3, 2*w//3)),\n",
        "                \"costophrenic_angles\": (slice(2*h//3, h), slice(w//4, 3*w//4)),\n",
        "                \"spine\": (slice(h//3, 2*h//3), slice(w//2-1, w//2+2)),\n",
        "                \"diaphragm\": (slice(2*h//3, h), slice(w//4, 3*w//4))\n",
        "            }\n",
        "\n",
        "            # Calculate attention score for each region\n",
        "            for region_name, (y_slice, x_slice) in regions.items():\n",
        "                if isinstance(attention_map, np.ndarray) and len(attention_map.shape) > 1:\n",
        "                    region_attention = attention_map[y_slice, x_slice]\n",
        "                    attention_score = np.mean(region_attention)\n",
        "                else:\n",
        "                    # Fallback if attention map is incorrect shape\n",
        "                    attention_score = 0.1\n",
        "\n",
        "                # Map score to anatomical region\n",
        "                self.detected_regions[region_name] = {\n",
        "                    \"attention_score\": float(attention_score),\n",
        "                    \"description\": self.anatomical_regions[region_name][\"description\"] if region_name in self.anatomical_regions else \"\",\n",
        "                    \"possible_conditions\": self.anatomical_regions[region_name][\"conditions\"] if region_name in self.anatomical_regions else []\n",
        "                }\n",
        "\n",
        "            # Find regions with highest attention\n",
        "            sorted_regions = sorted(self.detected_regions.items(), key=lambda x: x[1][\"attention_score\"], reverse=True)\n",
        "            self.primary_regions = [region[0] for region in sorted_regions[:3]]\n",
        "\n",
        "            debug_print(f\"Primary detected regions: {self.primary_regions}\")\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error detecting anatomical regions: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "\n",
        "    @performance_monitor\n",
        "    def hybrid_retrieve(self, text_query, image_embedding_tuple=None, k=Config.TOP_K_RETRIEVAL):\n",
        "        \"\"\"Retrieve relevant documents using text and/or image embeddings with enhanced algorithms\"\"\"\n",
        "        debug_print(f\"hybrid_retrieve called with query: {text_query[:50]}... and image_embedding_tuple: {'provided' if image_embedding_tuple else 'None'}\")\n",
        "\n",
        "        # Check cache first if caching is enabled\n",
        "        if Config.USE_CACHING:\n",
        "            cache_key = f\"hybrid_{hash(text_query)}_{hash(str(image_embedding_tuple)) if image_embedding_tuple else 'noimg'}\"\n",
        "            cached_result = self.cache.get(cache_key)\n",
        "            if cached_result is not None:\n",
        "                debug_print(\"Using cached hybrid retrieval results\")\n",
        "                return cached_result\n",
        "\n",
        "        try:\n",
        "            text_embedding = self.generate_text_embedding(text_query)\n",
        "\n",
        "            if image_embedding_tuple is not None:\n",
        "                debug_print(f\"Processing with image embedding, type: {type(image_embedding_tuple)}\")\n",
        "                # Convert tuple to array\n",
        "                image_embedding = np.array(image_embedding_tuple).reshape(1, -1)\n",
        "                debug_print(f\"Converted image embedding shape: {image_embedding.shape}\")\n",
        "\n",
        "                # Search using text embedding\n",
        "                text_distances, text_indices = self.text_index.search(text_embedding.astype('float32'), k)\n",
        "                debug_print(f\"Text search results: {len(text_indices[0])} indices\")\n",
        "\n",
        "                # Search using image embedding\n",
        "                image_distances, image_indices = self.image_index.search(image_embedding.astype('float32'), k)\n",
        "                debug_print(f\"Image search results: {len(image_indices[0])} indices\")\n",
        "\n",
        "                # Combine results with improved weighting\n",
        "                combined_indices = np.unique(np.concatenate([text_indices[0], image_indices[0]]))\n",
        "                debug_print(f\"Combined unique indices: {len(combined_indices)}\")\n",
        "\n",
        "                # Enhanced reranking with query-adaptive weighting\n",
        "                reranked_results = self.rerank_results(combined_indices, text_embedding, image_embedding)\n",
        "                debug_print(f\"Reranked results: {len(reranked_results)} items\")\n",
        "\n",
        "                # Process results\n",
        "                result = self.process_ranked_results(reranked_results)\n",
        "                debug_print(f\"hybrid_retrieve returning tuple with {len(result)} items - {type(result)}\")\n",
        "\n",
        "                # Cache the result if enabled\n",
        "                if Config.USE_CACHING:\n",
        "                    self.cache.put(cache_key, result)\n",
        "\n",
        "                return result\n",
        "            else:\n",
        "                debug_print(\"Processing text-only query\")\n",
        "\n",
        "                # Enhanced text retrieval with query expansion\n",
        "                distances, indices = self.text_index.search(text_embedding.astype('float32'), k)\n",
        "\n",
        "                # Add diversity to results if enabled\n",
        "                if Config.DYNAMIC_RERANKING:\n",
        "                    indices, distances = self.diversify_results(indices[0], distances[0], k)\n",
        "                    indices = np.array([indices])\n",
        "                    distances = np.array([distances])\n",
        "\n",
        "                retrieved_texts = [self.text_data['combined_text'].iloc[idx] for idx in indices[0]]\n",
        "                debug_print(f\"Retrieved {len(retrieved_texts)} texts for query\")\n",
        "                result = (retrieved_texts, indices[0], distances[0])\n",
        "\n",
        "                # Cache the result if enabled\n",
        "                if Config.USE_CACHING:\n",
        "                    self.cache.put(cache_key, result)\n",
        "\n",
        "                debug_print(f\"hybrid_retrieve returning tuple with {len(result)} items - {type(result)}\")\n",
        "                return result\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in hybrid_retrieve: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            # Return fallback values\n",
        "            empty_texts = [\"Error retrieving medical context.\"]\n",
        "            empty_indices = np.array([0])\n",
        "            empty_distances = np.array([1.0])\n",
        "            return empty_texts, empty_indices, empty_distances\n",
        "\n",
        "    def diversify_results(self, indices, distances, k):\n",
        "        \"\"\"Add diversity to search results using maximal marginal relevance\"\"\"\n",
        "        debug_print(\"Applying diversity optimization to search results\")\n",
        "\n",
        "        try:\n",
        "            # If we have fewer than threshold results, just return them\n",
        "            if len(indices) <= min(5, k // 2):\n",
        "                return indices, distances\n",
        "\n",
        "            # Use the first item as anchor\n",
        "            selected_indices = [indices[0]]\n",
        "            selected_distances = [distances[0]]\n",
        "            remaining_indices = list(indices[1:])\n",
        "            remaining_distances = list(distances[1:])\n",
        "\n",
        "            # Get embeddings for remaining documents\n",
        "            remaining_embeddings = np.array([self.text_embs[idx] for idx in remaining_indices])\n",
        "\n",
        "            # Add documents one by one with diversity consideration\n",
        "            while len(selected_indices) < k and remaining_indices:\n",
        "                # Calculate similarity between each remaining doc and the selected docs\n",
        "                max_similarities = []\n",
        "\n",
        "                # For each remaining document\n",
        "                for i in range(len(remaining_indices)):\n",
        "                    # Get its embedding\n",
        "                    embedding = remaining_embeddings[i]\n",
        "\n",
        "                    # Calculate similarities with all selected documents\n",
        "                    similarities = []\n",
        "                    for sel_idx in selected_indices:\n",
        "                        sel_embedding = self.text_embs[sel_idx]\n",
        "                        sim = np.dot(embedding, sel_embedding) / (np.linalg.norm(embedding) * np.linalg.norm(sel_embedding) + 1e-8)\n",
        "                        similarities.append(sim)\n",
        "\n",
        "                    # Get max similarity with any selected document\n",
        "                    max_sim = max(similarities) if similarities else 0\n",
        "                    max_similarities.append(max_sim)\n",
        "\n",
        "                # Apply Maximal Marginal Relevance formula:\n",
        "                # MMR = lambda * relevance - (1-lambda) * max_similarity\n",
        "                lambda_param = 0.7  # Balance between relevance and diversity\n",
        "                mmr_scores = [lambda_param * (1 - dist) - (1 - lambda_param) * sim\n",
        "                             for dist, sim in zip(remaining_distances, max_similarities)]\n",
        "\n",
        "                # Select item with highest MMR score\n",
        "                best_idx = np.argmax(mmr_scores)\n",
        "                selected_indices.append(remaining_indices[best_idx])\n",
        "                selected_distances.append(remaining_distances[best_idx])\n",
        "\n",
        "                # Remove selected item from remaining items\n",
        "                del remaining_indices[best_idx]\n",
        "                del remaining_distances[best_idx]\n",
        "                remaining_embeddings = np.delete(remaining_embeddings, best_idx, axis=0)\n",
        "\n",
        "            return np.array(selected_indices), np.array(selected_distances)\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in diversify_results: {str(e)}\")\n",
        "            # Fall back to original indices\n",
        "            return indices, distances\n",
        "\n",
        "    @performance_monitor\n",
        "    def rerank_results(self, indices, text_embedding, image_embedding):\n",
        "        \"\"\"Rerank results combining text and image similarity scores with enhanced weighting\"\"\"\n",
        "        debug_print(f\"Reranking {len(indices)} results\")\n",
        "        try:\n",
        "            reranked = []\n",
        "\n",
        "            # Check if query appears to be looking for specific conditions\n",
        "            # For medically specific queries, increase text weight\n",
        "            query_specificity = self.estimate_query_specificity(text_embedding)\n",
        "\n",
        "            # Adjust weights based on query specificity\n",
        "            text_weight = min(0.8, 0.5 + query_specificity * 0.3)\n",
        "            image_weight = 1.0 - text_weight\n",
        "\n",
        "            debug_print(f\"Reranking with text_weight={text_weight:.2f}, image_weight={image_weight:.2f}\")\n",
        "\n",
        "            # NOTE: Cannot directly compare text and image embeddings because of different dimensions\n",
        "            # (text: 768, image: 1024) - so we'll skip the coherence calculation\n",
        "\n",
        "            for idx in indices:\n",
        "                try:\n",
        "                    # Compute similarities separately (no cross-comparisons)\n",
        "                    text_sim = np.dot(text_embedding.flatten(), self.text_embs[idx].flatten()) / (\n",
        "                        np.linalg.norm(text_embedding) * np.linalg.norm(self.text_embs[idx]) + 1e-8)\n",
        "\n",
        "                    image_sim = np.dot(image_embedding.flatten(), self.image_embs[idx].flatten()) / (\n",
        "                        np.linalg.norm(image_embedding) * np.linalg.norm(self.image_embs[idx]) + 1e-8)\n",
        "\n",
        "                    # Compute combined score with dynamic weighting\n",
        "                    combined_score = text_weight * text_sim + image_weight * image_sim\n",
        "\n",
        "                    # Store for reranking\n",
        "                    reranked.append((idx, combined_score, text_sim, image_sim))\n",
        "                except Exception as item_e:\n",
        "                    debug_print(f\"Error processing item {idx}: {str(item_e)}\")\n",
        "                    # Use a fallback score\n",
        "                    reranked.append((idx, 0.0, 0.0, 0.0))\n",
        "\n",
        "            # Sort by combined score\n",
        "            sorted_results = sorted(reranked, key=lambda x: x[1], reverse=True)\n",
        "            debug_print(f\"Reranking complete, top score: {sorted_results[0][1] if sorted_results else 'N/A'}\")\n",
        "            return sorted_results\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in rerank_results: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            return [(idx, 0.0, 0.0, 0.0) for idx in indices[:5]]  # Return some indices with zero scores\n",
        "\n",
        "    def estimate_query_specificity(self, text_embedding):\n",
        "        \"\"\"Estimate query specificity based on embedding characteristics\"\"\"\n",
        "        try:\n",
        "            # Compute statistics on the embedding\n",
        "            mean_activation = np.mean(text_embedding)\n",
        "            std_activation = np.std(text_embedding)\n",
        "            max_activation = np.max(text_embedding)\n",
        "\n",
        "            # Higher std suggests more specific semantic content\n",
        "            # Improved formula with logistic function for smoother scaling\n",
        "            specificity = 1.0 / (1.0 + np.exp(-10 * (std_activation - 0.05)))\n",
        "            debug_print(f\"Query specificity estimate: {specificity:.2f}, std: {std_activation:.4f}\")\n",
        "            return specificity\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error estimating query specificity: {str(e)}\")\n",
        "            return 0.5  # Default to balanced weight\n",
        "\n",
        "    @performance_monitor\n",
        "    def process_ranked_results(self, reranked_results):\n",
        "        \"\"\"Process reranked results into texts, indices, and distances\"\"\"\n",
        "        debug_print(f\"Processing {len(reranked_results)} ranked results\")\n",
        "        try:\n",
        "            indices = [x[0] for x in reranked_results]\n",
        "            distances = [1 - x[1] for x in reranked_results]  # Convert similarity to distance\n",
        "\n",
        "            # Store text and image similarities for confidence analysis\n",
        "            self.text_similarities = [x[2] for x in reranked_results]\n",
        "            self.image_similarities = [x[3] for x in reranked_results]\n",
        "\n",
        "            # Get retrieved texts\n",
        "            retrieved_texts = [self.text_data['combined_text'].iloc[idx] for idx in indices]\n",
        "\n",
        "            # Apply PHI detection and anonymization\n",
        "            if Config.PHI_DETECTION_ENABLED:\n",
        "                retrieved_texts = [anonymize_text(text) for text in retrieved_texts]\n",
        "\n",
        "            # Enhanced context-based filtering\n",
        "            # Remove duplicative content using text similarity\n",
        "            if len(retrieved_texts) > 5:\n",
        "                filtered_texts, filtered_indices, filtered_distances = self.filter_duplicative_content(\n",
        "                    retrieved_texts, indices, distances\n",
        "                )\n",
        "                debug_print(f\"Filtered {len(retrieved_texts) - len(filtered_texts)} duplicative documents\")\n",
        "                retrieved_texts, indices, distances = filtered_texts, filtered_indices, filtered_distances\n",
        "\n",
        "            debug_print(f\"Processed ranked results: {len(retrieved_texts)} texts, indices shape: {len(indices)}, distances shape: {len(distances)}\")\n",
        "            return retrieved_texts, np.array(indices), np.array(distances)\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in process_ranked_results: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            return [\"Error processing results.\"], np.array([0]), np.array([1.0])\n",
        "\n",
        "    def filter_duplicative_content(self, texts, indices, distances, similarity_threshold=0.8):\n",
        "        \"\"\"Filter out duplicative content based on text similarity\"\"\"\n",
        "        if len(texts) <= 1:\n",
        "            return texts, indices, distances\n",
        "\n",
        "        try:\n",
        "            # Keep track of documents to retain\n",
        "            keep_indices = [0]  # Always include the top document\n",
        "\n",
        "            # Compare each document with the ones we've decided to keep\n",
        "            for i in range(1, len(texts)):\n",
        "                # Check if current document is too similar to any kept document\n",
        "                is_duplicate = False\n",
        "                for keep_idx in keep_indices:\n",
        "                    # Simple similarity check - word overlap ratio\n",
        "                    text1_words = set(texts[keep_idx].lower().split())\n",
        "                    text2_words = set(texts[i].lower().split())\n",
        "\n",
        "                    # Calculate Jaccard similarity\n",
        "                    if len(text1_words) > 0 and len(text2_words) > 0:\n",
        "                        intersection = len(text1_words.intersection(text2_words))\n",
        "                        union = len(text1_words) + len(text2_words) - intersection\n",
        "                        similarity = intersection / union\n",
        "\n",
        "                        if similarity > similarity_threshold:\n",
        "                            is_duplicate = True\n",
        "                            break\n",
        "\n",
        "                # If not a duplicate, keep it\n",
        "                if not is_duplicate:\n",
        "                    keep_indices.append(i)\n",
        "\n",
        "                # Don't process too many documents for efficiency\n",
        "                if len(keep_indices) >= Config.MAX_CONTEXT_DOCS:\n",
        "                    break\n",
        "\n",
        "            # Create filtered lists\n",
        "            filtered_texts = [texts[i] for i in keep_indices]\n",
        "            filtered_indices = [indices[i] for i in keep_indices]\n",
        "            filtered_distances = [distances[i] for i in keep_indices]\n",
        "\n",
        "            return filtered_texts, filtered_indices, filtered_distances\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in filter_duplicative_content: {str(e)}\")\n",
        "            return texts, indices, distances\n",
        "\n",
        "    def calibrate_confidence(self, raw_confidence):\n",
        "        \"\"\"Calibrate confidence scores using pre-computed mapping\"\"\"\n",
        "        try:\n",
        "            # Find closest calibration points in our mapping\n",
        "            keys = list(self.confidence_calibration.keys())\n",
        "            distances = [abs(k - raw_confidence) for k in keys]\n",
        "            closest_idx = distances.index(min(distances))\n",
        "            closest_key = keys[closest_idx]\n",
        "\n",
        "            # Get calibrated confidence\n",
        "            calibrated = self.confidence_calibration[closest_key]\n",
        "\n",
        "            # Interpolate if between calibration points\n",
        "            if raw_confidence > closest_key and closest_idx < len(keys) - 1:\n",
        "                next_key = keys[closest_idx + 1]\n",
        "                next_value = self.confidence_calibration[next_key]\n",
        "                # Linear interpolation\n",
        "                weight = (raw_confidence - closest_key) / (next_key - closest_key)\n",
        "                calibrated = self.confidence_calibration[closest_key] + weight * (next_value - self.confidence_calibration[closest_key])\n",
        "\n",
        "            debug_print(f\"Calibrated confidence: raw={raw_confidence:.2f}, calibrated={calibrated:.2f}\")\n",
        "            return calibrated\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in confidence calibration: {str(e)}\")\n",
        "            return raw_confidence  # Return raw value if calibration fails\n",
        "\n",
        "    def explain_confidence(self, distances):\n",
        "        \"\"\"\n",
        "        Generate confidence explanation from distances with enhanced calibration\n",
        "        \"\"\"\n",
        "        debug_print(f\"Explaining confidence for distances: {type(distances)}, shape: {getattr(distances, 'shape', 'N/A')}\")\n",
        "        try:\n",
        "            if not isinstance(distances, np.ndarray) or not distances.size:\n",
        "                return \"Confidence score unavailable due to no retrieved results.\"\n",
        "\n",
        "            # Print raw distance values for debugging\n",
        "            debug_print(f\"Distance stats - min: {np.min(distances):.4f}, max: {np.max(distances):.4f}, mean: {np.mean(distances):.4f}\")\n",
        "\n",
        "            # Modified calibration for the observed distance range (71-76)\n",
        "            # Transform to a more usable range\n",
        "            min_dist = np.min(distances[:5])  # Use top 5 results\n",
        "\n",
        "            # Map the observed range to confidence scores\n",
        "            if min_dist > 76:\n",
        "                confidence = 0.1  # Very low confidence\n",
        "            elif min_dist > 74:\n",
        "                confidence = 0.3  # Low confidence\n",
        "            elif min_dist > 72:\n",
        "                confidence = 0.5  # Moderate confidence\n",
        "            elif min_dist > 70:\n",
        "                confidence = 0.7  # Good confidence\n",
        "            else:\n",
        "                confidence = 0.9  # High confidence\n",
        "\n",
        "            # Apply calibration\n",
        "            confidence = self.calibrate_confidence(confidence)\n",
        "\n",
        "            # Enhanced explanation based on calibrated confidence\n",
        "            if confidence >= 0.9:\n",
        "                confidence_level = \"Very High\"\n",
        "                explanation = \"The system found very closely matching reference cases in the medical knowledge base.\"\n",
        "            elif confidence >= 0.75:\n",
        "                confidence_level = \"High\"\n",
        "                explanation = \"The system found strong matches in the medical knowledge base.\"\n",
        "            elif confidence >= 0.6:\n",
        "                confidence_level = \"Moderate\"\n",
        "                explanation = \"The system found reasonable matches, but with some differences from reference cases.\"\n",
        "            elif confidence >= 0.4:\n",
        "                confidence_level = \"Fair\"\n",
        "                explanation = \"The retrieved references only partially match this case.\"\n",
        "            else:\n",
        "                confidence_level = \"Low\"\n",
        "                explanation = \"The system found limited matches in the knowledge base for this case.\"\n",
        "\n",
        "            # Flag if confidence is below threshold\n",
        "            if confidence < Config.CONFIDENCE_THRESHOLD:\n",
        "                caution_note = \"Caution: Response confidence is below the reliability threshold. Please verify with other sources.\"\n",
        "            else:\n",
        "                caution_note = \"\"\n",
        "\n",
        "            # Format percentages\n",
        "            conf_pct = f\"{confidence * 100:.0f}%\"\n",
        "\n",
        "            full_explanation = f\"Confidence Level: {confidence_level} ({conf_pct})\\nExplanation: {explanation}\\n{caution_note}\"\n",
        "            debug_print(f\"Generated confidence explanation: {confidence_level} ({conf_pct})\")\n",
        "\n",
        "            return full_explanation\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error generating confidence explanation: {str(e)}\")\n",
        "            return \"Confidence assessment unavailable.\"\n",
        "\n",
        "    @performance_monitor\n",
        "    def handle_differentiation_query(self, query, retrieved_texts):\n",
        "        \"\"\"Handle differentiation queries dynamically based on retrieved knowledge\"\"\"\n",
        "        conditions_to_diff = []\n",
        "        all_conditions = get_mimic_cxr_conditions()\n",
        "\n",
        "        for condition in all_conditions:\n",
        "            if condition in query.lower():\n",
        "                conditions_to_diff.append(condition)\n",
        "\n",
        "        if len(conditions_to_diff) < 2:\n",
        "            print(f\"Warning: Differentiation query but found only {len(conditions_to_diff)} conditions: {conditions_to_diff}\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Differentiating between: {conditions_to_diff[:2]}\")\n",
        "\n",
        "        condition_texts = {c: [] for c in conditions_to_diff[:2]}\n",
        "\n",
        "        for text in retrieved_texts:\n",
        "            for condition in condition_texts.keys():\n",
        "                if condition in text.lower():\n",
        "                    sentences = re.split(r'[.!?]', text)\n",
        "                    for sentence in sentences:\n",
        "                        if condition in sentence.lower():\n",
        "                            condition_texts[condition].append(sentence)\n",
        "\n",
        "        if any(len(texts) == 0 for texts in condition_texts.values()):\n",
        "            print(\"Warning: Insufficient information from knowledge base for differentiation\")\n",
        "            return None\n",
        "\n",
        "        diff_text = f\"Differentiating {conditions_to_diff[0]} from {conditions_to_diff[1]}:\\n\\n\"\n",
        "\n",
        "        # Enhanced differentiation with structured comparison\n",
        "        diff_text += \"Key Differences:\\n\"\n",
        "\n",
        "        # For each condition, gather key characteristics\n",
        "        for condition, texts in condition_texts.items():\n",
        "            diff_text += f\"\\n{condition.capitalize()}:\\n\"\n",
        "\n",
        "            # Extract characteristic sentences\n",
        "            characteristic_sentences = set()\n",
        "            for text in texts[:5]:  # Use top 5 sentences\n",
        "                cleaned_text = text.strip()\n",
        "                if cleaned_text:\n",
        "                    characteristic_sentences.add(cleaned_text)\n",
        "\n",
        "            # Add bullet points for characteristics\n",
        "            for i, sentence in enumerate(characteristic_sentences):\n",
        "                if i < 5:  # Limit to 5 bullet points\n",
        "                    diff_text += f\"â€¢ {sentence}\\n\"\n",
        "\n",
        "        # Add direct comparison summary if available\n",
        "        for text in retrieved_texts:\n",
        "            if all(condition in text.lower() for condition in conditions_to_diff[:2]) and \"differ\" in text.lower():\n",
        "                diff_text += f\"\\nDirect Comparison:\\n{text}\\n\"\n",
        "                break\n",
        "\n",
        "        return diff_text\n",
        "\n",
        "    @performance_monitor\n",
        "    def generate_response(self, query, retrieved_texts, indices, distances):\n",
        "        \"\"\"Generate response from retrieved texts with improved formatting and caching\"\"\"\n",
        "        debug_print(f\"Generating response for query: {query[:50]}...\")\n",
        "\n",
        "        # Check cache for this exact query+context combination\n",
        "        cache_key = f\"response_{hash(query)}_{hash(str(retrieved_texts[:3]))}\"\n",
        "        cached_response = self.cache.get(cache_key)\n",
        "        if cached_response is not None:\n",
        "            debug_print(\"Using cached response\")\n",
        "            return cached_response\n",
        "\n",
        "        try:\n",
        "            # Clean and prepare context\n",
        "            cleaned_texts = []\n",
        "            for text in retrieved_texts[:5]:\n",
        "                # Remove any potential patient identifiers before using as context\n",
        "                cleaned_text = re.sub(r'\\[REDACTED\\]', '', text)\n",
        "                cleaned_texts.append(cleaned_text)\n",
        "\n",
        "            context = \"\\n\\nRetrieved Context:\\n\" + \"\\n\".join(cleaned_texts)\n",
        "            input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
        "\n",
        "            debug_print(\"Tokenizing input for generation\")\n",
        "            inputs = self.gen_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(self.device)\n",
        "\n",
        "            debug_print(\"Generating text with model\")\n",
        "            with torch.no_grad():\n",
        "                outputs = self.gen_model.generate(\n",
        "                    **inputs,\n",
        "                    max_length=250,\n",
        "                    num_beams=2,\n",
        "                    early_stopping=True,\n",
        "                    do_sample=True,  # Fix for temperature warning\n",
        "                    temperature=0.7,\n",
        "                    no_repeat_ngram_size=3  # Prevent repetition\n",
        "                )\n",
        "\n",
        "            debug_print(\"Decoding generated text\")\n",
        "            response = self.gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "            # Check if this is a differentiation query\n",
        "            if \"differentiate\" in query.lower():\n",
        "                debug_print(\"Processing differentiation query\")\n",
        "                diff_response = self.handle_differentiation_query(query, retrieved_texts)\n",
        "                if diff_response:\n",
        "                    response = diff_response\n",
        "\n",
        "            # Format the response\n",
        "            response = self.format_response(response, query)\n",
        "\n",
        "            # Generate confidence explanation\n",
        "            confidence_explanation = self.explain_confidence(distances)\n",
        "            response_with_confidence = response + \"\\n\\n\" + confidence_explanation\n",
        "\n",
        "            # Generate visual analysis if we have attention weights\n",
        "            visual_analysis = \"\"\n",
        "            if hasattr(self, 'last_attention_weights'):\n",
        "                debug_print(\"Generating visual analysis\")\n",
        "                visual_analysis = \"\\n\\n\" + self.generate_visual_analysis()\n",
        "\n",
        "            # Final response with PHI detection\n",
        "            final_response = anonymize_text(response_with_confidence + visual_analysis)\n",
        "\n",
        "            # Cache the result\n",
        "            self.cache.put(cache_key, final_response)\n",
        "\n",
        "            debug_print(f\"Response generation complete, length: {len(final_response)}\")\n",
        "            return final_response\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in generate_response: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            return f\"Error generating response: {str(e)}\"\n",
        "\n",
        "    def format_response(self, text, query):\n",
        "        \"\"\"Format response for consistency and readability with enhanced cleanup\"\"\"\n",
        "        # Remove any [REDACTED] markers\n",
        "        text = re.sub(r'\\[\\s*REDACTED\\s*\\]\\s*:', '', text)\n",
        "        text = re.sub(r'\\[\\s*REDACTED\\s*\\]', '', text)\n",
        "\n",
        "        # Remove numbered list formatting if present\n",
        "        text = re.sub(r'^\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "        # Clean up any improper line breaks or spacing\n",
        "        text = re.sub(r'\\s{2,}', ' ', text)\n",
        "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
        "\n",
        "        # Extract only the first complete report (to avoid mixing multiple reports)\n",
        "        # Look for patterns like \"FINDINGS: ... IMPRESSION: ...\" or similar\n",
        "        report_pattern = r'(FINDINGS?:.*?IMPRESSIONS?:.*?)(?:FINDINGS?:|$)'\n",
        "        report_match = re.search(report_pattern, text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        if report_match:\n",
        "            # Use only the first complete report\n",
        "            text = report_match.group(1).strip()\n",
        "\n",
        "        # Find and extract findings and impression\n",
        "        findings_match = re.search(r'FINDINGS?:(.*?)(?:IMPRESSIONS?:|$)', text, re.DOTALL | re.IGNORECASE)\n",
        "        impression_match = re.search(r'IMPRESSIONS?:(.*?)(?:FINDINGS?:|$)', text, re.DOTALL | re.IGNORECASE)\n",
        "\n",
        "        if findings_match or impression_match:\n",
        "            # Structure existing findings/impression sections\n",
        "            formatted = \"\"\n",
        "            if findings_match:\n",
        "                findings = findings_match.group(1).strip()\n",
        "                formatted += \"Findings:\\n\"\n",
        "\n",
        "                # Convert to bullet points if not already\n",
        "                if not re.search(r'^\\s*[â€¢\\-\\*]', findings, re.MULTILINE):\n",
        "                    # Split by sentences or semi-colons\n",
        "                    findings_points = []\n",
        "                    for sentence in re.split(r'[.;]\\s+', findings):\n",
        "                        if sentence.strip():\n",
        "                            findings_points.append(sentence.strip())\n",
        "\n",
        "                    for point in findings_points:\n",
        "                        if point and not point.endswith('.'):\n",
        "                            point += '.'\n",
        "                        if point:\n",
        "                            formatted += f\"â€¢ {point}\\n\"\n",
        "                else:\n",
        "                    formatted += findings + \"\\n\"\n",
        "\n",
        "            if impression_match:\n",
        "                impression = impression_match.group(1).strip()\n",
        "                formatted += f\"\\nImpression:\\n{impression}\"\n",
        "\n",
        "            text = formatted\n",
        "        else:\n",
        "            # If no structured sections found, create a simple findings section\n",
        "            if any(term in query.lower() for term in ['x-ray', 'xray', 'ct', 'mri', 'scan', 'imaging', 'radiograph']):\n",
        "                # Split by sentences\n",
        "                sentences = re.split(r'[.!?]\\s+', text)\n",
        "                formatted = \"Findings:\\n\"\n",
        "\n",
        "                # Take only the first 5 sentences to avoid mixing reports\n",
        "                for i, sentence in enumerate(sentences[:5]):\n",
        "                    sentence = sentence.strip()\n",
        "                    if sentence:\n",
        "                        if not sentence.endswith('.'):\n",
        "                            sentence += '.'\n",
        "                        formatted += f\"â€¢ {sentence}\\n\"\n",
        "\n",
        "                text = formatted\n",
        "\n",
        "        # Ensure text doesn't end mid-sentence\n",
        "        if text and not text.rstrip().endswith(('.', '!', '?')):\n",
        "            text = text.rstrip() + '.'\n",
        "\n",
        "        return text\n",
        "\n",
        "    @performance_monitor\n",
        "    def generate_visual_analysis(self):\n",
        "        \"\"\"Generate detailed analysis text from visual attention\"\"\"\n",
        "        debug_print(\"Generating visual analysis from attention weights\")\n",
        "\n",
        "        if not hasattr(self, 'last_attention_weights') or self.last_attention_weights is None:\n",
        "            return \"Visual Analysis: Not available for this query.\"\n",
        "\n",
        "        try:\n",
        "            # Get attention map and region information\n",
        "            attention_map = self.last_attention_weights\n",
        "\n",
        "            # Start with basic information (removed [REDACTED] label)\n",
        "            analysis = \"Regions of Interest:\\n\"\n",
        "\n",
        "            # Add anatomical region analysis if available\n",
        "            if hasattr(self, 'detected_regions') and self.detected_regions:\n",
        "                # Get top 3 regions with highest attention\n",
        "                top_regions = sorted(self.detected_regions.items(), key=lambda x: x[1][\"attention_score\"], reverse=True)[:3]\n",
        "\n",
        "                for region_name, region_data in top_regions:\n",
        "                    # Format the region name for display\n",
        "                    display_name = region_name.replace('_', ' ').title()\n",
        "\n",
        "                    # Add region description\n",
        "                    analysis += f\"â€¢ {region_data['description']}: \"\n",
        "\n",
        "                    # Add attention level description\n",
        "                    score = region_data['attention_score']\n",
        "                    if score > 0.5:\n",
        "                        attention_level = \"high\"\n",
        "                    elif score > 0.2:\n",
        "                        attention_level = \"moderate\"\n",
        "                    else:\n",
        "                        attention_level = \"low\"\n",
        "\n",
        "                    analysis += f\"{attention_level} attention\"\n",
        "\n",
        "                    # Add potential findings if available\n",
        "                    if region_data['possible_conditions'] and len(region_data['possible_conditions']) > 0:\n",
        "                        conditions = [c.capitalize() for c in region_data['possible_conditions'][:2]]\n",
        "                        analysis += f\" (may indicate {' or '.join(conditions)})\"\n",
        "\n",
        "                    analysis += \"\\n\"\n",
        "\n",
        "                # Add asymmetry analysis\n",
        "                if hasattr(self, 'region_attention'):\n",
        "                    left_attention = (self.region_attention['upper_left'] + self.region_attention['lower_left']) / 2\n",
        "                    right_attention = (self.region_attention['upper_right'] + self.region_attention['lower_right']) / 2\n",
        "\n",
        "                    if abs(left_attention - right_attention) > 0.15:  # Significant asymmetry\n",
        "                        dominant_side = \"left\" if left_attention > right_attention else \"right\"\n",
        "                        analysis += f\"\\nAsymmetry: The {dominant_side} side shows significantly more findings of interest.\\n\"\n",
        "            else:\n",
        "                # Fallback if detailed region data isn't available\n",
        "                analysis += \"â€¢ Areas of high attention indicate potential findings requiring clinical correlation.\\n\"\n",
        "\n",
        "            return analysis\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in generate_visual_analysis: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            return \"Visual Analysis: Attention heatmap shows regions of interest in the image.\"\n",
        "\n",
        "    @performance_monitor\n",
        "    def evaluate_response(self, query, response, image_id=None):\n",
        "        \"\"\"\n",
        "        Evaluate response quality using MIMIC-CXR ground truth when available\n",
        "\n",
        "        Args:\n",
        "            query: The user query string\n",
        "            response: The generated response\n",
        "            image_id: The ID of the image being analyzed (if applicable)\n",
        "\n",
        "        Returns:\n",
        "            tuple: (precision, recall, f1)\n",
        "        \"\"\"\n",
        "        debug_print(f\"Evaluating response for query: {query[:50]}...\")\n",
        "\n",
        "        try:\n",
        "            conditions = get_mimic_cxr_conditions()\n",
        "            condition_synonyms = get_condition_synonyms()\n",
        "\n",
        "            response_mentions = {c: False for c in conditions}\n",
        "            response_lower = response.lower()\n",
        "\n",
        "            for condition in conditions:\n",
        "                if condition in response_lower:\n",
        "                    response_mentions[condition] = True\n",
        "                    continue\n",
        "\n",
        "                if condition in condition_synonyms:\n",
        "                    for synonym in condition_synonyms[condition]:\n",
        "                        if synonym in response_lower:\n",
        "                            response_mentions[condition] = True\n",
        "                            break\n",
        "\n",
        "            mentioned_conditions = [c for c, mentioned in response_mentions.items() if mentioned]\n",
        "            primary_condition = mentioned_conditions[0] if mentioned_conditions else None\n",
        "\n",
        "            ground_truth = self.get_ground_truth(\n",
        "                image_id=image_id,\n",
        "                condition=primary_condition,\n",
        "                query=query\n",
        "            )\n",
        "\n",
        "            debug_print(f\"Using ground truth: '{ground_truth[:100]}...'\")\n",
        "\n",
        "            ground_truth_mentions = {c: False for c in conditions}\n",
        "            ground_truth_lower = ground_truth.lower()\n",
        "\n",
        "            for condition in conditions:\n",
        "                if condition in ground_truth_lower:\n",
        "                    ground_truth_mentions[condition] = True\n",
        "                    continue\n",
        "\n",
        "                if condition in condition_synonyms:\n",
        "                    for synonym in condition_synonyms[condition]:\n",
        "                        if synonym in ground_truth_lower:\n",
        "                            ground_truth_mentions[condition] = True\n",
        "                            break\n",
        "\n",
        "            pred_labels = [int(response_mentions[c]) for c in conditions]\n",
        "            true_labels = [int(ground_truth_mentions[c]) for c in conditions]\n",
        "\n",
        "            debug_print(\"Condition detection results:\")\n",
        "            for i, condition in enumerate(conditions):\n",
        "                debug_print(f\"- {condition}: pred={bool(pred_labels[i])}, true={bool(true_labels[i])}\")\n",
        "\n",
        "            if sum(true_labels) > 0:\n",
        "                if \"differentiate\" in query.lower():\n",
        "                    diff_conditions = [c for c in conditions if c in query.lower()]\n",
        "                    if len(diff_conditions) >= 2:\n",
        "                        resp_has_both = all(response_mentions[c] for c in diff_conditions[:2])\n",
        "                        truth_has_both = all(ground_truth_mentions[c] for c in diff_conditions[:2])\n",
        "\n",
        "                        precision = 1.0 if resp_has_both == truth_has_both else 0.0\n",
        "                        recall = 1.0 if resp_has_both == truth_has_both else 0.0\n",
        "                        f1 = 1.0 if resp_has_both == truth_has_both else 0.0\n",
        "\n",
        "                        self.eval_metrics[\"precision\"].append(precision)\n",
        "                        self.eval_metrics[\"recall\"].append(recall)\n",
        "                        self.eval_metrics[\"f1\"].append(f1)\n",
        "\n",
        "                        debug_print(f\"Differentiation query evaluation - Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
        "                        return precision, recall, f1\n",
        "\n",
        "                precision = precision_score(true_labels, pred_labels, zero_division=0)\n",
        "                recall = recall_score(true_labels, pred_labels, zero_division=0)\n",
        "                f1 = f1_score(true_labels, pred_labels, zero_division=0)\n",
        "            else:\n",
        "                if sum(pred_labels) == 0:\n",
        "                    precision = 1.0\n",
        "                    recall = 1.0\n",
        "                    f1 = 1.0\n",
        "                else:\n",
        "                    precision = 0.0\n",
        "                    recall = 0.0\n",
        "                    f1 = 0.0\n",
        "\n",
        "            self.eval_metrics[\"precision\"].append(precision)\n",
        "            self.eval_metrics[\"recall\"].append(recall)\n",
        "            self.eval_metrics[\"f1\"].append(f1)\n",
        "\n",
        "            debug_print(f\"Evaluation results - Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")\n",
        "            return precision, recall, f1\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in evaluate_response: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            return 0.0, 0.0, 0.0\n",
        "\n",
        "    def get_evaluation_metrics(self):\n",
        "        \"\"\"Get overall evaluation metrics with proper averaging\"\"\"\n",
        "        debug_print(\"Getting evaluation metrics\")\n",
        "\n",
        "        if not self.eval_metrics[\"precision\"]:\n",
        "            debug_print(\"No evaluation metrics collected yet\")\n",
        "            return {\"avg_precision\": 0.0, \"avg_recall\": 0.0, \"avg_f1\": 0.0, \"evaluations\": 0}\n",
        "\n",
        "        avg_precision = np.mean(self.eval_metrics[\"precision\"])\n",
        "        avg_recall = np.mean(self.eval_metrics[\"recall\"])\n",
        "        avg_f1 = np.mean(self.eval_metrics[\"f1\"])\n",
        "\n",
        "        debug_print(f\"Metrics collected: {len(self.eval_metrics['precision'])} evaluations\")\n",
        "        debug_print(f\"Avg precision: {avg_precision:.2f}, Avg recall: {avg_recall:.2f}, Avg F1: {avg_f1:.2f}\")\n",
        "\n",
        "        metrics = {\n",
        "            \"avg_precision\": round(float(avg_precision), 2),\n",
        "            \"avg_recall\": round(float(avg_recall), 2),\n",
        "            \"avg_f1\": round(float(avg_f1), 2),\n",
        "            \"evaluations\": len(self.eval_metrics[\"precision\"])\n",
        "        }\n",
        "\n",
        "        # Add performance metrics if available\n",
        "        if self.performance_metrics:\n",
        "            perf_summary = {}\n",
        "            for func_name, times in self.performance_metrics.items():\n",
        "                if times:\n",
        "                    perf_summary[func_name] = {\n",
        "                        \"avg_time\": round(float(np.mean(times)), 3),\n",
        "                        \"max_time\": round(float(np.max(times)), 3),\n",
        "                        \"calls\": len(times)\n",
        "                    }\n",
        "            metrics[\"performance\"] = perf_summary\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def get_synthesized_ground_truth(self, condition=None, query=None):\n",
        "        \"\"\"Generate synthesized ground truth when MIMIC-CXR data is not available\"\"\"\n",
        "        debug_print(\"Using synthesized ground truth (fallback)\")\n",
        "\n",
        "        # For differentiation queries\n",
        "        if query and (\"differentiate\" in query.lower() or \"difference\" in query.lower()):\n",
        "            conditions = [c for c in get_mimic_cxr_conditions() if c in query.lower()]\n",
        "            if len(conditions) >= 2:\n",
        "                return f\"The key radiographic differences between {conditions[0]} and {conditions[1]} relate to their distribution, appearance, and associated findings.\"\n",
        "\n",
        "        # For specific condition queries\n",
        "        if condition:\n",
        "            return f\"The typical radiographic appearance of {condition} includes its characteristic findings as documented in radiology literature.\"\n",
        "\n",
        "        # General fallback\n",
        "        return \"Accurate chest X-ray interpretation requires assessment of all anatomical structures and potential pathological findings based on established radiographic principles.\"\n",
        "\n",
        "    def get_ground_truth(self, image_id=None, condition=None, query=None):\n",
        "        \"\"\"Get ground truth from MIMIC-CXR index\n",
        "\n",
        "        Args:\n",
        "            image_id: ID of the image if available\n",
        "            condition: Specific condition to get ground truth for\n",
        "            query: Query text to find relevant ground truth\n",
        "\n",
        "        Returns:\n",
        "            str: Ground truth text from MIMIC-CXR\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not self.has_mimic_data:\n",
        "                return self.get_synthesized_ground_truth(condition, query)\n",
        "\n",
        "            if image_id and image_id in self.mimic_index[\"by_image_id\"]:\n",
        "                record = self.mimic_index[\"by_image_id\"][image_id]\n",
        "                if record[\"impression\"]:\n",
        "                    return anonymize_text(record[\"impression\"])\n",
        "                elif record[\"findings\"]:\n",
        "                    return anonymize_text(record[\"findings\"])\n",
        "\n",
        "            if condition and condition in self.mimic_index[\"by_condition\"]:\n",
        "                condition_examples = self.mimic_index[\"by_condition\"][condition]\n",
        "                if condition_examples:\n",
        "                    random_id = random.choice(condition_examples)\n",
        "                    record = self.mimic_index[\"by_image_id\"][random_id]\n",
        "                    if record[\"impression\"]:\n",
        "                        return anonymize_text(record[\"impression\"])\n",
        "                    elif record[\"findings\"]:\n",
        "                        return anonymize_text(record[\"findings\"])\n",
        "\n",
        "            return self.get_synthesized_ground_truth(condition, query)\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error accessing ground truth: {str(e)}\")\n",
        "            return self.get_synthesized_ground_truth(condition, query)\n",
        "\n",
        "    @performance_monitor\n",
        "    def process_image_query(self, image):\n",
        "        \"\"\"Process an image query with explicit attention weight extraction\"\"\"\n",
        "        try:\n",
        "            debug_print(\"Starting process_image_query with attention weight focus\")\n",
        "            image_embedding = self.generate_image_embedding(image)\n",
        "            debug_print(f\"Image embedding generated, shape: {image_embedding.shape}\")\n",
        "\n",
        "            if not hasattr(self, 'last_attention_weights'):\n",
        "                debug_print(\"Warning: No attention weights were captured during embedding generation!\")\n",
        "                self.last_attention_weights = np.ones((7, 7))\n",
        "            else:\n",
        "                debug_print(f\"Attention weights captured, shape: {self.last_attention_weights.shape}\")\n",
        "\n",
        "            image_embedding_list = image_embedding.flatten().tolist()\n",
        "            image_embedding_tuple = tuple(image_embedding_list)\n",
        "            debug_print(f\"Converted to tuple of length: {len(image_embedding_tuple)}\")\n",
        "\n",
        "            try:\n",
        "                debug_print(\"Calling hybrid_retrieve\")\n",
        "                hybrid_result = self.hybrid_retrieve(\"Describe this chest X-ray\", image_embedding_tuple)\n",
        "                if isinstance(hybrid_result, tuple) and len(hybrid_result) == 3:\n",
        "                    retrieved_texts, indices, distances = hybrid_result\n",
        "                else:\n",
        "                    debug_print(f\"WARNING: hybrid_retrieve returned unexpected value: {type(hybrid_result)}\")\n",
        "                    retrieved_texts = [\"No medical context available.\"]\n",
        "                    indices = np.array([0])\n",
        "                    distances = np.array([1.0])\n",
        "            except Exception as e:\n",
        "                debug_print(f\"Error in hybrid_retrieve: {e}\")\n",
        "                retrieved_texts = [\"Error retrieving medical context.\"]\n",
        "                indices = np.array([0])\n",
        "                distances = np.array([1.0])\n",
        "\n",
        "            debug_print(\"Generating response\")\n",
        "            # Generate only the findings and impression, without visual analysis\n",
        "            query = \"Describe this chest X-ray\"\n",
        "\n",
        "            # Clean and prepare context\n",
        "            cleaned_texts = []\n",
        "            for text in retrieved_texts[:5]:\n",
        "                # Remove any potential patient identifiers before using as context\n",
        "                cleaned_text = re.sub(r'\\[REDACTED\\]', '', text)\n",
        "                cleaned_texts.append(cleaned_text)\n",
        "\n",
        "            context = \"\\n\\nRetrieved Context:\\n\" + \"\\n\".join(cleaned_texts)\n",
        "            input_text = f\"Query: {query}\\nContext: {context}\\nAnswer:\"\n",
        "\n",
        "            # Generate the medical findings\n",
        "            try:\n",
        "                inputs = self.gen_tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.gen_model.generate(\n",
        "                        **inputs,\n",
        "                        max_length=250,\n",
        "                        num_beams=2,\n",
        "                        early_stopping=True,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        no_repeat_ngram_size=3\n",
        "                    )\n",
        "\n",
        "                response = self.gen_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                # Format the response without including visual analysis again\n",
        "                medical_findings = self.format_response(response, query)\n",
        "            except Exception as e:\n",
        "                debug_print(f\"Error in text generation: {str(e)}\")\n",
        "                medical_findings = \"Unable to generate findings due to an error.\"\n",
        "\n",
        "            # Generate the visual analysis separately\n",
        "            visual_analysis = self.generate_visual_analysis()\n",
        "\n",
        "            # Combine findings with visual analysis\n",
        "            combined_response = f\"{medical_findings}\\n\\n{visual_analysis}\"\n",
        "\n",
        "            debug_print(\"Capturing attention weights for visualization\")\n",
        "            attention_weights = np.array(self.last_attention_weights).copy()\n",
        "            debug_print(f\"Attention weights shape for return: {attention_weights.shape}\")\n",
        "\n",
        "            # Evaluate response\n",
        "            self.evaluate_response(query, combined_response, None)\n",
        "\n",
        "            debug_print(\"Returning from process_image_query with response and attention weights\")\n",
        "            return combined_response, attention_weights\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error in process_image_query: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            return f\"Error processing image: {str(e)}\", np.ones((7, 7))\n",
        "\n",
        "    @performance_monitor\n",
        "    def process_text_query(self, text):\n",
        "        \"\"\"Process a text query\"\"\"\n",
        "        debug_print(f\"process_text_query called with text: {text[:50]}...\")\n",
        "        try:\n",
        "            debug_print(\"Retrieving documents for text query\")\n",
        "            retrieved_texts, indices, distances = self.hybrid_retrieve(text)\n",
        "            debug_print(f\"Retrieved {len(retrieved_texts)} texts\")\n",
        "\n",
        "            debug_print(\"Generating response\")\n",
        "            response = self.generate_response(text, retrieved_texts, indices, distances)\n",
        "            debug_print(f\"Response generated, length: {len(response)}\")\n",
        "\n",
        "            debug_print(\"Returning from process_text_query\")\n",
        "            return response, None\n",
        "\n",
        "        except Exception as e:\n",
        "            debug_print(f\"ERROR in process_text_query: {str(e)}\")\n",
        "            debug_print(traceback.format_exc())\n",
        "            return (f\"Error processing text query: {str(e)}\", None)\n",
        "\n",
        "    def free_memory(self):\n",
        "        \"\"\"Free memory by clearing caches and moving models to CPU\"\"\"\n",
        "        debug_print(\"Freeing memory...\")\n",
        "        try:\n",
        "            # Clear cache\n",
        "            if hasattr(self, 'cache'):\n",
        "                self.cache.clear()\n",
        "                debug_print(\"Cache cleared\")\n",
        "\n",
        "            # Move models to CPU to free GPU memory\n",
        "            if torch.cuda.is_available():\n",
        "                # Check if we're using lazy loading\n",
        "                if Config.LAZY_LOADING:\n",
        "                    if hasattr(self, 'text_model') and isinstance(self.text_model, LazyModel):\n",
        "                        self.text_model.unload()\n",
        "                        debug_print(\"Unloading model dmis-lab/biobert-v1.1\")\n",
        "\n",
        "                    if hasattr(self, 'gen_model') and isinstance(self.gen_model, LazyModel):\n",
        "                        self.gen_model.unload()\n",
        "                        debug_print(\"Unloading model <your_path>\")\n",
        "                else:\n",
        "                    # Move regular models to CPU\n",
        "                    if hasattr(self, 'text_model'):\n",
        "                        self.text_model = self.text_model.cpu()\n",
        "\n",
        "                    if hasattr(self, 'image_model'):\n",
        "                        self.image_model = self.image_model.cpu()\n",
        "\n",
        "                    if hasattr(self, 'gen_model'):\n",
        "                        self.gen_model = self.gen_model.cpu()\n",
        "\n",
        "                # Clear CUDA cache\n",
        "                torch.cuda.empty_cache()\n",
        "                debug_print(\"Models moved to CPU and CUDA cache cleared\")\n",
        "\n",
        "            # Clear any stored embeddings or attention weights\n",
        "            if hasattr(self, 'last_attention_weights'):\n",
        "                del self.last_attention_weights\n",
        "\n",
        "            if hasattr(self, 'raw_activations'):\n",
        "                del self.raw_activations\n",
        "\n",
        "            if hasattr(self, 'region_attention'):\n",
        "                del self.region_attention\n",
        "\n",
        "            if hasattr(self, 'detected_regions'):\n",
        "                del self.detected_regions\n",
        "\n",
        "            if hasattr(self, 'primary_regions'):\n",
        "                del self.primary_regions\n",
        "\n",
        "            # Run garbage collection\n",
        "            gc.collect()\n",
        "\n",
        "            debug_print(\"Memory freed successfully\")\n",
        "        except Exception as e:\n",
        "            debug_print(f\"Error freeing memory: {str(e)}\")\n",
        "\n",
        "# === Utility Functions for Gradio Interface ===\n",
        "def process_query(text_query=None, image_file=None):\n",
        "    \"\"\"Process user query with improved MIMIC-CXR based evaluation\"\"\"\n",
        "    global rag\n",
        "\n",
        "    debug_print(f\"process_query called with text_query: {'provided' if text_query else 'None'}, image_file: {'provided' if image_file is not None else 'None'}\")\n",
        "\n",
        "    if not text_query and image_file is None:\n",
        "        return \"Please provide a query or image.\", None\n",
        "\n",
        "    try:\n",
        "        image_id = None\n",
        "        if image_file is not None and hasattr(image_file, 'filename'):\n",
        "            filename = os.path.basename(image_file.filename)\n",
        "            match = re.search(r'p\\d+/p\\d+/s\\d+/\\d+', filename)\n",
        "            if match:\n",
        "                image_id = match.group(0)\n",
        "                debug_print(f\"Extracted image_id: {image_id}\")\n",
        "\n",
        "        if image_file is not None:\n",
        "            debug_print(f\"Processing image query, image_file type: {type(image_file)}\")\n",
        "\n",
        "            # Handle different image input types\n",
        "            if not isinstance(image_file, Image.Image):\n",
        "                debug_print(f\"Converting {type(image_file)} to PIL Image\")\n",
        "                if isinstance(image_file, np.ndarray):\n",
        "                    # NumPy array\n",
        "                    image_file = Image.fromarray(image_file)\n",
        "                elif isinstance(image_file, str) and os.path.exists(image_file):\n",
        "                    # File path\n",
        "                    image_file = Image.open(image_file)\n",
        "                else:\n",
        "                    # Try generic conversion\n",
        "                    image_file = Image.fromarray(np.array(image_file))\n",
        "\n",
        "            # Ensure image is in correct format (RGB)\n",
        "            if image_file.mode != 'RGB':\n",
        "                debug_print(f\"Converting image from {image_file.mode} to RGB\")\n",
        "                image_file = image_file.convert('RGB')\n",
        "\n",
        "            result = rag.process_image_query(image_file)\n",
        "\n",
        "            if isinstance(result, tuple) and len(result) >= 2:\n",
        "                response_text = result[0]\n",
        "                attention_weights = result[1]\n",
        "            else:\n",
        "                debug_print(f\"Warning: Expected tuple result, got {type(result)}\")\n",
        "                response_text = str(result)\n",
        "                attention_weights = None\n",
        "\n",
        "            attention_map = None\n",
        "            if attention_weights is not None:\n",
        "                attention_map = visualize_attention_map(image_file, attention_weights)\n",
        "\n",
        "            # Internal evaluation (not shown to user)\n",
        "            rag.evaluate_response(\n",
        "                \"Describe this chest X-ray\",\n",
        "                response_text,\n",
        "                image_id\n",
        "            )\n",
        "\n",
        "            # Clean up response - remove confidence explanation\n",
        "            response_text = remove_confidence_section(response_text)\n",
        "\n",
        "            return response_text, attention_map\n",
        "\n",
        "        elif text_query and len(text_query.strip()) >= 3:\n",
        "            debug_print(\"Processing text query\")\n",
        "            result = rag.process_text_query(text_query)\n",
        "\n",
        "            if isinstance(result, tuple) and len(result) >= 1:\n",
        "                response_text = result[0]\n",
        "            else:\n",
        "                response_text = str(result)\n",
        "\n",
        "            # Internal evaluation (not shown to user)\n",
        "            rag.evaluate_response(\n",
        "                text_query,\n",
        "                response_text,\n",
        "                None\n",
        "            )\n",
        "\n",
        "            # Clean up response - remove confidence explanation\n",
        "            response_text = remove_confidence_section(response_text)\n",
        "\n",
        "            return response_text, None\n",
        "\n",
        "        else:\n",
        "            return \"Please enter a detailed query or upload an image.\", None\n",
        "\n",
        "    except Exception as e:\n",
        "        error_message = f\"Error processing query: {str(e)}\"\n",
        "        debug_print(error_message)\n",
        "        debug_print(traceback.format_exc())\n",
        "        return error_message, None\n",
        "\n",
        "def remove_confidence_section(text):\n",
        "    \"\"\"Remove confidence information from response\"\"\"\n",
        "    # Pattern to match confidence section - matches standard formats used\n",
        "    confidence_patterns = [\n",
        "        r'Confidence Level:.*?(?=\\n\\n|$)',\n",
        "        r'Explanation:.*?(?=\\n\\n|$)'\n",
        "    ]\n",
        "\n",
        "    result = text\n",
        "    for pattern in confidence_patterns:\n",
        "        result = re.sub(pattern, '', result, flags=re.DOTALL)\n",
        "\n",
        "    # Clean up any duplicate newlines created by removal\n",
        "    result = re.sub(r'\\n{3,}', '\\n\\n', result)\n",
        "\n",
        "    return result.strip()\n",
        "\n",
        "def visualize_attention_map(image, attention_weights):\n",
        "    \"\"\"Create a visualization of model attention on the image with enhanced anatomical mapping\"\"\"\n",
        "    debug_print(\"visualize_attention_map called with:\")\n",
        "    debug_print(f\"- Image type: {type(image)}, size: {image.size if hasattr(image, 'size') else 'unknown'}\")\n",
        "    debug_print(f\"- Attention weights type: {type(attention_weights)}\")\n",
        "\n",
        "    try:\n",
        "        if attention_weights is None:\n",
        "            debug_print(\"ERROR: attention_weights is None\")\n",
        "            return None\n",
        "\n",
        "        if not isinstance(image, Image.Image):\n",
        "            debug_print(f\"Converting {type(image)} to PIL Image\")\n",
        "            image = Image.fromarray(np.array(image))\n",
        "\n",
        "        img_array = np.array(image)\n",
        "        debug_print(f\"Image array shape: {img_array.shape}\")\n",
        "\n",
        "        if hasattr(attention_weights, 'shape'):\n",
        "            debug_print(f\"Attention weights shape: {attention_weights.shape}\")\n",
        "            if len(attention_weights.shape) == 1:\n",
        "                size = int(np.sqrt(attention_weights.shape[0]))\n",
        "                attention_weights = attention_weights.reshape(size, size)\n",
        "                debug_print(f\"Reshaped attention weights to {attention_weights.shape}\")\n",
        "            elif len(attention_weights.shape) > 2:\n",
        "                attention_weights = np.mean(attention_weights, axis=0)\n",
        "                debug_print(f\"Averaged attention weights to shape {attention_weights.shape}\")\n",
        "        else:\n",
        "            debug_print(\"ERROR: attention_weights has no shape attribute\")\n",
        "            attention_weights = np.ones((224, 224))\n",
        "\n",
        "        attention_weights = np.abs(attention_weights)\n",
        "\n",
        "        # Apply Gaussian smoothing to make heatmap more natural\n",
        "        attention_weights = cv2.GaussianBlur(attention_weights, (5, 5), 0)\n",
        "\n",
        "        target_size = (img_array.shape[1], img_array.shape[0])\n",
        "        debug_print(f\"Resizing attention map to {target_size}\")\n",
        "        attention_map = cv2.resize(attention_weights, target_size)\n",
        "\n",
        "        # Enhanced normalization with histogram equalization for better contrast\n",
        "        min_val = np.min(attention_map)\n",
        "        max_val = np.max(attention_map)\n",
        "        if max_val > min_val:\n",
        "            # Apply non-linear transformation to enhance contrast\n",
        "            attention_map = np.power((attention_map - min_val) / (max_val - min_val), 0.7)\n",
        "        else:\n",
        "            attention_map = np.zeros_like(attention_map)\n",
        "\n",
        "        debug_print(f\"Normalized attention map shape: {attention_map.shape}, min: {np.min(attention_map)}, max: {np.max(attention_map)}\")\n",
        "\n",
        "        # Use improved colormap for medical imaging\n",
        "        heatmap = cv2.applyColorMap((attention_map * 255).astype(np.uint8), cv2.COLORMAP_INFERNO)\n",
        "        debug_print(f\"Heatmap shape: {heatmap.shape}\")\n",
        "\n",
        "        # Prepare the background image\n",
        "        if len(img_array.shape) == 2:\n",
        "            debug_print(\"Converting grayscale image to BGR\")\n",
        "            img_bgr = cv2.cvtColor(img_array, cv2.COLOR_GRAY2BGR)\n",
        "        elif len(img_array.shape) == 3:\n",
        "            if img_array.shape[2] == 4:\n",
        "                debug_print(\"Converting RGBA image to BGR\")\n",
        "                img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGBA2BGR)\n",
        "            elif img_array.shape[2] == 3:\n",
        "                debug_print(\"Converting RGB image to BGR\")\n",
        "                img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n",
        "            else:\n",
        "                debug_print(f\"Unexpected image shape: {img_array.shape}\")\n",
        "                img_bgr = img_array\n",
        "        else:\n",
        "            debug_print(f\"Unexpected image dimensions: {img_array.shape}\")\n",
        "            img_bgr = img_array\n",
        "\n",
        "        if img_bgr.shape[:2] != heatmap.shape[:2]:\n",
        "            debug_print(f\"Warning: Image shape {img_bgr.shape[:2]} and heatmap shape {heatmap.shape[:2]} mismatch\")\n",
        "            heatmap = cv2.resize(heatmap, (img_bgr.shape[1], img_bgr.shape[0]))\n",
        "\n",
        "        # Enhanced overlay with adaptive transparency\n",
        "        # This makes the heatmap more transparent in low-attention areas\n",
        "        alpha = np.clip(attention_map * 0.7, 0.2, 0.7)  # Adaptive transparency\n",
        "        alpha = np.expand_dims(alpha, axis=2)\n",
        "        alpha = np.repeat(alpha, 3, axis=2)  # Repeat for RGB channels\n",
        "\n",
        "        # Create the blended image\n",
        "        overlay = img_bgr.astype(float) * (1 - alpha) + heatmap.astype(float) * alpha\n",
        "        overlay = np.clip(overlay, 0, 255).astype(np.uint8)\n",
        "\n",
        "        # Add anatomical region markers if available\n",
        "        if hasattr(rag, 'detected_regions') and hasattr(rag, 'primary_regions'):\n",
        "            # Define region coordinates (normalized to image size)\n",
        "            h, w = img_bgr.shape[:2]\n",
        "            regions = {\n",
        "                \"upper_right_lung\": (int(w*0.75), int(h*0.25)),\n",
        "                \"upper_left_lung\": (int(w*0.25), int(h*0.25)),\n",
        "                \"middle_right_lung\": (int(w*0.75), int(h*0.5)),\n",
        "                \"lower_right_lung\": (int(w*0.75), int(h*0.75)),\n",
        "                \"lower_left_lung\": (int(w*0.25), int(h*0.75)),\n",
        "                \"heart\": (int(w*0.5), int(h*0.5)),\n",
        "                \"hilar\": (int(w*0.5), int(h*0.4)),\n",
        "                \"costophrenic_angles\": (int(w*0.5), int(h*0.85)),\n",
        "                \"spine\": (int(w*0.5), int(h*0.5)),\n",
        "                \"diaphragm\": (int(w*0.5), int(h*0.75))\n",
        "            }\n",
        "\n",
        "            # Add markers for primary regions\n",
        "            for region_name in rag.primary_regions:\n",
        "                if region_name in regions:\n",
        "                    x, y = regions[region_name]\n",
        "                    cv2.circle(overlay, (x, y), 10, (255, 255, 255), -1)\n",
        "                    cv2.circle(overlay, (x, y), 10, (0, 0, 0), 2)\n",
        "\n",
        "                    # Add label\n",
        "                    label = region_name.replace('_', ' ').title()\n",
        "                    cv2.putText(overlay, label,\n",
        "                              (x - 50, y - 15),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                              0.5, (255, 255, 255), 2)\n",
        "\n",
        "        result = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)\n",
        "        debug_print(f\"Final visualization shape: {result.shape}\")\n",
        "\n",
        "        try:\n",
        "            cv2.imwrite('debug_visualization.jpg', cv2.cvtColor(result, cv2.COLOR_RGB2BGR))\n",
        "            debug_print(\"Debug visualization saved to debug_visualization.jpg\")\n",
        "        except Exception as save_error:\n",
        "            debug_print(f\"Error saving debug visualization: {save_error}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        debug_print(f\"Error in visualize_attention_map: {str(e)}\")\n",
        "        debug_print(traceback.format_exc())\n",
        "        if isinstance(image, Image.Image):\n",
        "            return np.array(image)\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "# === Examples for Gradio Interface ===\n",
        "def get_examples():\n",
        "    \"\"\"Generate examples for the Gradio interface\"\"\"\n",
        "    examples = [\n",
        "        [\"What does pleural effusion look like on a chest X-ray?\"],\n",
        "        [\"How to differentiate pulmonary edema from pneumonia?\"],\n",
        "        [\"What radiographic findings are typical for tuberculosis?\"],\n",
        "        [\"What are the key features of cardiomegaly on X-ray?\"],\n",
        "        [\"How do atelectasis and pneumothorax differ radiographically?\"],\n",
        "        [\"What should I look for to identify lung nodules?\"],\n",
        "        [\"Describe radiographic signs of COPD on chest X-ray.\"],\n",
        "        [\"What features suggest malignancy in a lung nodule?\"]\n",
        "    ]\n",
        "\n",
        "    return examples[:Config.EXAMPLES_TO_SHOW]\n",
        "\n",
        "# === Main execution ===\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\\n===== STARTING MEDIQUERY RAG SYSTEM (ENHANCED 2025) =====\")\n",
        "    print(f\"Current date and time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"MediQuery version: 2.0.1 (March 9, 2025)\")\n",
        "    print(f\"Developer: Tanishk\")\n",
        "\n",
        "    try:\n",
        "        print(\"Mounting Google Drive...\")\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "        print(\"Google Drive mounted successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting drive: {str(e)}\")\n",
        "        print(\"Continuing without mounted drive. Some features may not be available.\")\n",
        "\n",
        "    try:\n",
        "        print(\"Initializing RAG system...\")\n",
        "        rag = MediQueryRAG(\n",
        "            knowledge_base_dir=\"<your_path>\",\n",
        "            finetuned_model_path=\"<your_path>\"\n",
        "        )\n",
        "        print(\"RAG system initialized successfully\")\n",
        "\n",
        "        print(\"Testing RAG with simple text query...\")\n",
        "        test_result = rag.process_text_query(\"What does pneumonia look like?\")\n",
        "        print(f\"Test query result type: {type(test_result)}, is tuple: {isinstance(test_result, tuple)}\")\n",
        "        if isinstance(test_result, tuple):\n",
        "            print(f\"Test response length: {len(test_result[0]) if test_result[0] else 'empty'}\")\n",
        "\n",
        "        print(\"Creating Gradio interface...\")\n",
        "        with gr.Blocks(theme=gr.themes.Base()) as interface:\n",
        "            gr.Markdown(\"# MediQuery - Advanced Chest X-Ray Analysis\")\n",
        "            gr.Markdown(\"\"\"\n",
        "            Ask about chest X-rays or upload an image for analysis.\n",
        "\n",
        "            **Developed by:** Tanishk | **Version:** 2.0.1 (March 2025)\n",
        "\n",
        "            *This tool uses a fine-tuned RAG system on the MIMIC-CXR dataset to provide medical imaging analysis.*\n",
        "            \"\"\")\n",
        "\n",
        "            with gr.Row():\n",
        "                with gr.Column(scale=1):\n",
        "                    text_input = gr.Textbox(\n",
        "                        label=\"Enter your medical query\",\n",
        "                        placeholder=\"e.g., 'What does pneumonia look like?'\",\n",
        "                        lines=2\n",
        "                    )\n",
        "                    image_input = gr.Image(\n",
        "                        label=\"Upload a chest X-ray image\",\n",
        "                        type=\"pil\",\n",
        "                        height=Config.IMAGE_HEIGHT\n",
        "                    )\n",
        "                    submit_btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "\n",
        "                    gr.Examples(\n",
        "                        get_examples(),\n",
        "                        inputs=[text_input],\n",
        "                        label=\"Example Queries\"\n",
        "                    )\n",
        "\n",
        "                with gr.Column(scale=2):\n",
        "                    text_output = gr.Textbox(\n",
        "                        label=\"Medical Analysis\",\n",
        "                        lines=15\n",
        "                    )\n",
        "                    image_output = gr.Image(\n",
        "                        label=\"Regions of Interest Visualization\",\n",
        "                        height=Config.IMAGE_HEIGHT\n",
        "                    )\n",
        "\n",
        "            submit_btn.click(\n",
        "                fn=process_query,\n",
        "                inputs=[text_input, image_input],\n",
        "                outputs=[text_output, image_output]\n",
        "            )\n",
        "\n",
        "            gr.Markdown(\"\"\"\n",
        "            ### Important Notes:\n",
        "            - This tool is for educational purposes only and is not a substitute for professional medical advice\n",
        "            - Results should be verified by qualified healthcare professionals\n",
        "            - For real medical concerns, please consult with a doctor\n",
        "\n",
        "            *MIMIC-CXR dataset is used under appropriate research permissions.*\n",
        "            \"\"\")\n",
        "\n",
        "        print(\"Launching Gradio interface...\")\n",
        "        interface.launch(debug=True)\n",
        "\n",
        "        metrics = rag.get_evaluation_metrics()\n",
        "        print(f\"Final Evaluation Metrics: {metrics}\")\n",
        "\n",
        "        # Release resources when done\n",
        "        rag.free_memory()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Critical error initializing or running RAG system: {str(e)}\")\n",
        "        print(traceback.format_exc())"
      ],
      "metadata": {
        "id": "W3HqN05sNGAS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}